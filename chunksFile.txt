Chunk 1:
Page Content: 1
Parameter-Efficient Fine-Tuning for Large Models:
A Comprehensive Survey
Zeyu Han1, Chao Gao 2, Jinyang Liu 1, Jeff (Jun) Zhang 3, and Sai Qian Zhang* 4
1Northeastern University 2University of California, Riverside 3Arizona State University
4New York University
{han.zeyu,liu.jinyan}@northeastern.edu, cgao037@ucr.edu, jeffzhang@asu.edu, sai.zhang@nyu.edu
Abstract—Large models represent a groundbreaking advance-
ment in multiple application fields, enabling remarkable achieve-
ments across various tasks. However, their unprecedented scale
comes with significant computational costs. These models, often
consisting of billions of parameters, require vast amounts of
computational resources for execution. Especially, the expansive
scale and computational demands pose considerable challenges
when customizing them for particular downstream tasks, particu-
larly over the hardware platforms constrained by computational
capabilities.
Parameter Efficient Fine-Tuning (PEFT) provides a practical
Metadata: {}
--------------------------------------------------

Chunk 2:
Page Content: capabilities.
Parameter Efficient Fine-Tuning (PEFT) provides a practical
solution by efficiently adjusting the large models over the various
downstream tasks. In particular, PEFT refers to the process of
adjusting the parameters of a pre-trained large model to adapt
it to a specific task or domain while minimizing the number
of additional parameters introduced or computational resources
required. This approach is particularly important when dealing
with large-scale language models with high parameter counts,
as fine-tuning these models from scratch can be computationally
expensive and resource-intensive, posing considerable challenges
in the supporting system platform design.
In this survey, we present comprehensive studies of various
PEFT algorithms, examining their performance and computa-
tional overhead. Moreover, we provide an overview of applica-
tions developed using different PEFT algorithms and discuss
common techniques employed to mitigate computation costs
Metadata: {}
--------------------------------------------------

Chunk 3:
Page Content: common techniques employed to mitigate computation costs
for PEFT. In addition to providing an extensive survey from
an algorithmic standpoint, we also examine various real-world
system designs to investigate the implementation costs associated
with different PEFT approaches. This survey serves as a valuable
resource for researchers aiming to understand both the PEFT al-
gorithm and its system implementation, offering detailed insights
into recent advancements and practical applications.
Index Terms—Large Language Model, Parameter-Efficient
Fine-tuning, Computer System, Distributed System.
I. I NTRODUCTION
Large Models (LMs) have recently captured considerable
public interest. Their ability to understand context and nuances
enables them to proficiently handle diverse tasks across mul-
tiple domains, including natural language processing (NLP),
computer vision (CV), etc. In the field of NLP, Large Lan-
guage Models (LLMs) have achieved significant advance-
Metadata: {}
--------------------------------------------------

Chunk 4:
Page Content: guage Models (LLMs) have achieved significant advance-
ments across various tasks including text generation [1], [2],
translation [3], [4], personalized chat-bots [5], [6], [7], and
summarization [8], demonstrating remarkable proficiency.
* Corresponding author
Earlier studies [1] have suggested that LLMs exhibit high
levels of generalization, enabling them to apply their acquired
knowledge to new tasks not included in their original training.
This capability is commonly known as zero-shot learning .
Nevertheless, fine-tuning remains essential to further enhance
LLMs for optimal performance on new user datasets and tasks.
Due to its scale, a widely adopted strategy for fine-tuning
LLMs involves adjusting a limited number of LLM parame-
ters while keeping the remainder unchanged. This technique,
termed Parameter-Efficient-Fine-Tuning (PEFT), involves se-
lectively adjusting a small proportion of their parameters while
keeping the rest unaltered. Furthermore, the application of
Metadata: {}
--------------------------------------------------

Chunk 5:
Page Content: keeping the rest unaltered. Furthermore, the application of
PEFT extends beyond the realm of NLP and quickly attracts
interest in the CV community for handling fine-tuning vision
models with large parameters, such as Vision Transformers
(ViT) and diffusion models, as well as disciplinary models
such as vision-language models.
In this survey, we systematically review and categorize
recent advancements in PEFT algorithms as well as the system
implementation costs associated with various PEFT algorithms
across diverse scenarios. Figure 1 presents the overview con-
tent for this survey. In section II, we present some fundamental
concepts for LLM and PEFT, including computational flow
for LLM, basic knowledge of PEFT, commonly used datasets
and tasks, and evaluation benchmarks. We categorize all
types of PEFT algorithms in Section III according to their
computational flow. In Section III-A, we detail additive algo-
rithms that either introduce new weight parameters or modify
Metadata: {}
--------------------------------------------------

Chunk 6:
Page Content: rithms that either introduce new weight parameters or modify
activations. Algorithms that only require fine-tuning of existing
parameters are categorized as selective approaches, which are
introduced in Section III-B. In Section III-C, we explore
reparameterized PEFT, which constructs a (low- dimensional)
reparameterization of original model parameters for training
while transforming the weights back to maintain the inference
speed. Additionally, there exist algorithms that combine the
above techniques, and we have classified these as hybrid
approaches, elaborating on them in Section III-D. We also
investigate strategies for further reducing the computational
complexity of different PEFT algorithms, including KV-cache
management, pruning, quantization, and memory optimization,
in Section IV.
In Section V, we expand the scope of this survey beyond
the computational perspective to involve various potential
application scenarios. Specifically, we explore innovations that
Metadata: {}
--------------------------------------------------

Chunk 7:
Page Content: application scenarios. Specifically, we explore innovations that
arXiv:2403.14608v7  [cs.LG]  16 Sep 2024
2
Background
Computational 
flow for LLM
PEFT 
Taxonomy
Selective
PEFT
Additive
PEFT
System Design
Challenge
System Design 
for PEFT
Centralized PEFT 
Serving System
PEFT for 
LLMs
Distributed PEFT 
Training System
Hybrid
PEFT
PEFT
overview Reparameterized 
PEFT
Efficient PEFT
Design
KV-cache 
Management for 
PEFT Efficiency
PEFT Pruning
PEFT 
Quantization
Memory-efficient 
PEFT
Parallel PEFT 
Training System
Apply PEFT for 
other Applications
PEFT for 
ViTs
PEFT for 
VLAs
PEFT for 
Diffusion Models
Downstream
tasks
Section 2 Section 3 Section 4 Section 5 Section 6
2.1
2.2
2.3
3.1
3.2
3.3
3.4
4.1
4.2
4.3
4.4
5.1
5.2
5.3
5.4
6.1
6.2
6.3
6.4
Fig. 1: A content overview covered in the survey.
applying PEFT techniques to different model architecture,
including LLMs (Section V-A), Vision Transformer (Sec-
tion V-B), Vision-Language alignment models (Section V-C),
Metadata: {}
--------------------------------------------------

Chunk 8:
Page Content: tion V-B), Vision-Language alignment models (Section V-C),
and Diffusion models (Section V-D), for varied downstream
tasks, underscoring PEFT’s versatility and applicability in a
range of scenarios. After that, in Section VI, we explore the
system design challenge for PEFT methods. The discussion
includes three advanced system solutions for practical PEFT
deployment: PEFT query serving (Section VI-B), distributed
tuning (Section VI-C), and concurrent PEFT tuning (Sec-
tion VI-D). Finally, in Section VII, we summarize our survey
and propose several potential future directions from both
algorithmic and systemic perspectives, aiming to offer valuable
insights for further research and development in the field.
II. B ACKGROUND
In this section, we first discussed the computation flow of
LLM, including its fundamental components, computational
complexity, and the flow of computations it involves as a case
study. We then provide a brief overview of different PEFT
algorithms in section II-B.
Metadata: {}
--------------------------------------------------

Chunk 9:
Page Content: study. We then provide a brief overview of different PEFT
algorithms in section II-B.
A. Computation flow for LLaMA
In order to gain a deeper understanding of LLM and other
Transformer-based models, we employ LLaMA-7B, a cutting-
edge open-source LLM model, to scrutinize the architecture
of LLM as well as Transformer. As shown in Figure 2 (a),
LLaMA consists of three major components: an embedding
block, a stack of decoder blocks, and a head block which
consists of linear and softmax layer. The embedding layer’s
primary role is to transform unstructured textual information,
into chunks of discrete numerical vectors ( tokens) to facilitate
subsequent processing. The embedded tokens are then deliv-
ered to the decoder layers for further processing. Each LLaMA
decoder is composed of two fundamental components: Multi-
head Self-Attention (MSA) and Feedforward Network (FFN).
In the MSA module, each of the tokens will be clustered by
an attention map obtained by a dot production between two
Metadata: {}
--------------------------------------------------

Chunk 10:
Page Content: an attention map obtained by a dot production between two
linear mappings of the input tokens. Then the grouped tokens
will be further processed by a Feedforward Neural network.
Additionally, Root Mean Square Layer Normalization (RM-
SNorm) [9] is adopted in LLaMA as a replacement for Layer
Normalization to ensure efficient training.
LLM distinguishes itself from other deep neural network
(DNN) models such as convolutional neural networks (CNN)
in two significant ways. Firstly, LLM exhibits an inherent
autoregressive nature, necessitating multiple iterations to com-
plete the generation task. Moreover, LLM incorporates an
attention mechanism, a component with computational com-
plexity that scales quadratically with the length of the inputs.
On the other hand, the inherent computation characteristic of
LLM lies in the attention blocks inside each decoder layer.
Figure 2 (c) depicts the high-level overview of the computation
flow in the attention block.
Metadata: {}
--------------------------------------------------

Chunk 11:
Page Content: Figure 2 (c) depicts the high-level overview of the computation
flow in the attention block.
During the inference process, each decoder takes a three-
dimensional tensor x P Rbˆlˆd as the input tokens. The
input tokens are first multiplied with three weight matrices
WQ, WK, and WV , producing the output referred to as
query(Q), key( K) and value( V ). Given the MSA module’s
inability to recognize positional data and the inherent auto-
regressive nature of LLMs, the query and key will undergo
a process using Rotary Positional Embedding [10] (RoPE,
denoted as Rp.qin Eq 1) to encode the position information.
Subsequently, the key and value will be combined with prior
tokens.
After the positional embedding, the intermediate activation
will then undergo a series of multiplication, softmax, and
residual addition to generate MSA output as described in Eq 9.
To be noted here, dk in the equation refers to the number of
feature dimensions in the multi-head attention mechanism.
Metadata: {}
--------------------------------------------------

Chunk 12:
Page Content: feature dimensions in the multi-head attention mechanism.
Q, K, V“RpWqxq, RpWkxq, Wvx (1)
SApxq“ Softmax p QKT
?dhead
qV (2)
MSA pxq“r SA1pxq; SA2pxq; . . .; SAkpxqsWo (3)
The SA output will then be forwarded to the FFN blocks
for further processing. The FFN block will have another three
3
<BOS>
FFN
SA
LLaMA
ML
ML is
LLaMA LLaMA
is
awesome
LLaMA
awesome <EOS>
Decoder
Decoder
Decoder
Linear &
Softmax
…
Q K V
LoRA
FC
FC
ReLU
Adapter
Decoder
SA
FFN
Wdown
Wup
Input tokensPrompt
(c)
Embedding
(b)(a)
Fig. 2: (a) LLaMA architecture. (b) LLaMA auto-regressive pattern. (c) Three common PEFT operations. All the learnable
components are highlighted in red, while the frozen components are highlighted in grey. LoRA is applied on all the Query, Key,
and Value blocks. The adapter targets the FFN module. Soft-Prompt focused on tuning the input activation of each decoder.
We only show one decoder for illustration simplicity.
matrices Wup, Wdown, and Wgate and the computation can be
illustrated by:
Metadata: {}
--------------------------------------------------

Chunk 13:
Page Content: matrices Wup, Wdown, and Wgate and the computation can be
illustrated by:
F F NLLaMa pxq“ WuppSiLU pWgatexqdp Wdownxqq` x,
(4)
where x denotes the input of the FFN layer, and SiLU
is the nonlinear function used in LLaMA. In the original
Transformer, the FFN block can be demonstrated by:
F F NTransfomer pxq“ WuppReLUpWdownxqq` x. (5)
The output of the last decoder layer will be sent to a
linear layer, which then generates a probability distribution
spanning the complete vocabulary to predict the next token in
the sequence. The produced token will then be concatenated
with the previous tokens and used as the input for the next
round of processing. This generating process repeats in an
auto-regressive manner until a full sequence of tokens, referred
to as a completion, is produced (Figure 2 (b)). For training, the
computation flow is similar to that for inference, except that
the generated sentences are directly compared to the ground
Metadata: {}
--------------------------------------------------

Chunk 14:
Page Content: the generated sentences are directly compared to the ground
truth output and generate the training loss. Gradients will then
be computed across the LLM weights to minimize this training
loss.
To analyze the computation cost and memory overhead
in LLM, we also set a series of parameters used in later
section III. Table I shows the parameter size and computation
dimension in the LLaMA-7B model as a starting example.
LLM models generate tokens (words) one for each round,
depicted in Fig 2, based on the previous prompt (input) and
previously generated sequence. This process will be repeated
until the model outputs hits and termination token. To accel-
erate the inference process in LLM models, people take the
strategy of storing the previous Keys and Values in the Key-
Value cache (KV-cache), so they don’t need to recalculate
them for each new token. Mathematically, we can represent
the total decoders’ KV-cache memory cost in equation 6. In
Metadata: {}
--------------------------------------------------

Chunk 15:
Page Content: the total decoders’ KV-cache memory cost in equation 6. In
the equation, l and b are the context length and batch size
and L refers to the number of layers. The dhead is the head
dimension and nhead is the number of heads.
Size “L ˆ2 ˆb ˆl ˆdhead ˆnhead (6)
B. Overview on Parameter Efficient Fine Tuning
Fine-tuning remains essential to enhance LLM performance
on unseen user datasets and tasks. With the size of the model
growing (e.g. 1.5B in GPT-2 to 175B in GPT-3), standard
full fine-tuning paradigm requires thousands of GPU work
in parallel, which is highly inefficient and unsustainable. A
type of algorithm has been raised namely Parameter-efficient
fine-tuning (PEFT) which aims to tune minimal parameters
to achieve better performance over full tuning on downstream
tasks.
In parallel developments, large-scale pre-trained models in
vision and multimodal domains have also demonstrated their
effective representational learning capabilities, enabling adap-
Metadata: {}
--------------------------------------------------

Chunk 16:
Page Content: effective representational learning capabilities, enabling adap-
tation from large datasets to smaller ones or across various data
modalities through fine-tuning. Consequently, this capability
has made PEFT increasingly attractive to the wider research
community.
We categorized the PEFT algorithms into additive, selec-
tive, reparameterized, and hybrid fine-tuning based on their
operations. As Figure 3 depicts, three major additive fine-
tuning algorithms are normally used: (1) Adapter; (2) Soft
Prompt; (3) Others. They differ from each other in terms of the
different additional tunable modules or parameters. Selective
fine-tuning, on the other hand, doesn’t require any additional
parameters, it selects a small subset of parameters from the
backbone model and only makes them tunable while keeping
the majority of parameters untouched during fine-tuning on
downstream tasks. We categorized selective fine-tuning based
4
Metadata: {}
--------------------------------------------------

Chunk 17:
Page Content: downstream tasks. We categorized selective fine-tuning based
4
TABLE I: Configuration parameters and computation operation for LLaMA-7B architecture
Operation Weights Symbol Weights Dimension Input Tensor Dimension Complexity
Eq. 1 WQ , WK , WV d ˆk ˆ d
k b ˆl ˆd Oplq
Eq. 2 - - b ˆl ˆ3 ˆk ˆ d
k Opl2 q
Eq. 3 Wo d ˆd b ˆl ˆd Oplq
Eq. 4 Wup , Wdown , Wgate d ˆ4d b ˆl ˆd OR l ˆb ˆ4d Oplq
on the grouping of chosen parameters: (1) Unstructural Mask-
ing; and (2) Structural Masking. Reparametrization represents
transforming model parameters between two equivalent forms.
Specifically, reparametrized fine-tuning introduces addi-
tional low-rank trainable parameters during training, which
are then integrated with the original model for inference. This
approach is categorized into two main strategies: (1) Low-
rank Decomposition, and (2) LoRA Derivatives. Hybrid fine-
tuning explores the design spaces of different PEFT methods
and combines their advantages.
C. Downstream Tasks for LLM Evaluation
Metadata: {}
--------------------------------------------------

Chunk 18:
Page Content: and combines their advantages.
C. Downstream Tasks for LLM Evaluation
Two types of tasks have been widely used for LLM eval-
uation, the first type is the General Language Understand-
ing Evaluation (GLUE) [11] benchmark, which integrates
nine sentence or sentence-pair language understanding tasks
(CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, and
WNLI), chosen for their diversity in dataset sizes, text genres,
and difficulty levels, and is based on established existing
datasets. It also includes a diagnostic dataset specifically
designed to evaluate and analyze model performance across
various linguistic phenomena inherent in natural language.
Additionally, it features a public leaderboard to track perfor-
mance on the benchmark and a dashboard to visualize model
performance on the diagnostic set.
The other type of dataset that has been used in recent
LLM papers is common sense reasoning which integrated
into our study caters to a variety of research facets: (1)
Metadata: {}
--------------------------------------------------

Chunk 19:
Page Content: into our study caters to a variety of research facets: (1)
OpenBookQA [12] is curated to foster research in advanced
question-answering, delving into a profound understanding
of both the subject matter and the language in which it
is articulated. (2) PIQA [13] primarily emphasizes everyday
scenarios, demonstrating a predilection for unconventional
solutions. (3) Social IQA [14] emerges as a novel question-
answering benchmark tailored for gauging social common-
sense intelligence. (4) HellaSwag [15] serves as a dataset, the
essence of which is to ascertain the capability of machines in
aptly concluding sentences. (5) BoolQ [16] is a dataset dedi-
cated to question-answering, particularly for binary responses
(yes/no queries). (6) WinoGrande [17] is introduced as a fresh
compilation, encompassing a substantial 44,000 problems. (7)
ARC-easy [18] presents itself as a novel dataset constituting
genuine grade-school level multiple-choice science questions,
Metadata: {}
--------------------------------------------------

Chunk 20:
Page Content: genuine grade-school level multiple-choice science questions,
designed to invigorate research in intricate question-answering.
(8) ARC-challenges [18], distinctively, encompasses solely
those questions that were inaccurately addressed by both a
retrieval-based algorithm and a word co-occurrence algorithm.
Image recognition is the primary benchmark and application
for vision models, exemplified by benchmarks such as fine-
grained visual categorization (FGVC) and visual task adapta-
tion benchmark (VTAB). Beyond image classification, video
action recognition is another key application area, involving
datasets like Kinetics-400 [19], SSv2 [20], and HMDB51 [21].
Additionally, PEFT has been utilized for dense prediction
tasks, using datasets like MSCOCO [22], ADE20K [23], and
PASCAL VOC [24].
D. Evaluation Benchmarks for PEFT
To help readers evaluate the performance differences be-
tween various PEFT methods under a unified standard, a com-
Metadata: {}
--------------------------------------------------

Chunk 21:
Page Content: tween various PEFT methods under a unified standard, a com-
prehensive benchmark is essential. Next, we discuss several
commonly used benchmarks.
From the algorithmic perspective, [25] benchmarks the
performance of several PEFT algorithms across more than
100 NLP tasks and conducts systematic experiments based on
criteria such as performance, convergence, efficiency, combin-
ability, scalability, and transferability. Similarly, [26] and [27]
have also established targeted benchmarks to evaluate different
PEFT algorithms.
From the system perspective, three commonly used bench-
marks are outlined below to evaluate system performance.
The first benchmark is the ShareGPT dataset [28], which
includes real-world interactions with OpenAI’s ChatGPT. It
encompasses a broad spectrum of conversational queries and
responses that are representative of typical user interactions
with large language models (LLMs). This dataset is vital
for evaluating the system’s ability to manage diverse and
Metadata: {}
--------------------------------------------------

Chunk 22:
Page Content: for evaluating the system’s ability to manage diverse and
realistic conversational requirements, focusing on the accuracy
of responses and efficiency in handling requests.
The second benchmark involves the Microsoft Azure Func-
tion Trace from the years 2019 and 2021 [29], containing
logs from serverless computing activities via Azure Functions.
While these logs are from a general serverless computing con-
text rather than LLM-specific applications, they offer insights
into the computational demands driven by events. These traces
simulate the arrival patterns and workload intensities that LLM
systems might face, including irregular and peak demands,
thus acting as practical proxies for LLM inference tasks.
The third benchmark is based on the Gamma process [30],
a prevalent approach in simulations to model the timing of
incoming requests in queueing and service systems. This
method facilitates the creation of workloads with varied arrival
Metadata: {}
--------------------------------------------------

Chunk 23:
Page Content: method facilitates the creation of workloads with varied arrival
rates and patterns, producing synthetic, yet realistic request
5
PEFT Methods for PLMs
Additive
Fine-tuning
Adapter-based
Fine-tuning
Adapter Design Serial Adapter [31], Parallel Adapter [32], CIAT [33], CoDA [34]
Multi-task Adaptation AdapterFusion [35], AdaMix [36], PHA [37], AdapterSoup [38], MerA [39], Hyperformer [40]
Soft Prompt-based
Fine-tuning
Soft Prompt Design Prefix-tuning [41], Prefix-Propagation [42], p-tuning v2 [43], APT [44], p-tuning [45],
prompt-tuning [46], Xprompt [47], IDPG [48], LPT [49], SPT [50], APrompt [51]
Training Speedup SPoT [52], TPT [53], InfoPrompt [54], PTP [55], IPT [56], SMoP [57], DePT [58]
Others (IA)3 [59], MoV [60], SSF [61], IPA [62]
Selective
Fine-tuning
Unstructural
Masking U-Diff pruning [63], U-BitFit [64], PaFi [65], FishMask [66], Fish-Dip [67], LT-SFT [68], SAM [69], Child-tuning [70]
Metadata: {}
--------------------------------------------------

Chunk 24:
Page Content: Structural Masking S-Diff pruning [63], S-BitFit [64], FAR [71], Bitfit [72], Xattn Tuning [73], SPT [74]
Reparameterized
Fine-tuning
Low-rank
Decomposition Intrinsic SAID [75], LoRA [76], Compacter [77], KronA [78], KAdaptation [79], HiWi [65], VeRA [80], DoRA [81]
LoRA Derivatives
Dynamic Rank DyLoRA [82], AdaLoRA [83], SoRA [84], CapaBoost [85], AutoLoRA [86]
LoRA Improvement Laplace-LoRA [87], LoRA Dropout [88], PeriodicLoRA [89], LoRA+ [90], MoSLoRA [91]
Multiple LoRA LoRAHub [92], MOELoRA [93], MoLORA [60], MoA [94], MoLE [95], MixLoRA [96]
Hybrid
Fine-tuning UniPELT [97], S4 [98], MAM Adapter [32], NOAH [99], AUTOPEFT [100], LLM-Adapters [101], S3PET [102]
Fig. 3: Taxonomy of Parameter-Efficient Fine-Tuning Methods for Large Models.
Output
Combine
Frozen Learnable
Input
Output
Input
(c) Reparameterization PEFT(a) Additive PEFT (b) Selective PEFT
Merge
Output
Input Input (train)
Fig. 4: Different types of PEFT algorithms.
Metadata: {}
--------------------------------------------------

Chunk 25:
Page Content: Merge
Output
Input Input (train)
Fig. 4: Different types of PEFT algorithms.
scenarios that a system could encounter during actual opera-
tions. Such synthetic workloads are crucial for testing system
performance under controlled conditions that resemble real-
world user activity.
III. PEFT T AXONOMY
The PEFT strategies can be broadly classified into four
categories: additive PEFT (Section III-A), which modifies
the model architecture by injecting new trainable modules
or parameters; selective PEFT (Section III-B), which makes
a subset of parameters trainable during fine-tuning; repa-
rameterized PEFT (Section III-C), which constructs a (low-
dimensional) reparameterization of the original model param-
eters for training, then equivalently transforms it back for
inference; and hybrid PEFT (Section III-D), which combines
advantages from different PEFT methods to build a unified
PEFT model. An overview of different types of PEFT algo-
rithms is depicted in Figure 4.
A. Additive PEFT
Metadata: {}
--------------------------------------------------

Chunk 26:
Page Content: rithms is depicted in Figure 4.
A. Additive PEFT
Standard full fine-tuning entails substantial computational
expenses and could also potentially harm the model’s gener-
alization ability. To mitigate this problem, a widely employed
approach is to maintain the pre-trained backbone unchanged
and introduce only a minimal number of trainable parameters
that are strategically positioned within the model architecture.
While fine-tuning for a specific downstream task, only the
weights of these additional modules or parameters are updated,
which results in a substantial reduction in storage, memory,
and computational resource requirements. Due to their char-
acteristic of adding parameters, these techniques can be termed
as Additive Tuning, as shown in Figure 4 (a). Next, we discuss
several popular Additive PEFT algorithms.
1) Adapters: Adapter approaches involve the insertion of
small adapter layers within Transformer blocks. Typically, an
Metadata: {}
--------------------------------------------------

Chunk 27:
Page Content: small adapter layers within Transformer blocks. Typically, an
adapter layer consists of a down-projection matrix Wdown P
Rrˆd, followed by a non-linear activation function σp¨q, and
an up-projection matrix Wup P Rdˆr. In this context, d
represents the dimension of the hidden layer, and r serves
as the bottleneck dimension, which is a hyperparameter used
in configuring the adapters. Denote hin as the input to the
adapter, the computation within the adapter module (with
residual) can be summarized as follows:
Adapterpxq“ WupσpWdownxq` x. (7)
The concept of adapters in the field of NLP was initially
introduced by Serial Adapter [31] as shown in Figure 5
(a). In their approach, each Transformer block is enhanced
by adding two adapter modules, with one positioned after
the self-attention layer and the other after the FFN layer,
respectively. Subsequent research has aimed to address the
additional computational cost associated with adapter layers.
Metadata: {}
--------------------------------------------------

Chunk 28:
Page Content: additional computational cost associated with adapter layers.
A modified framework AdapterFusion [35] was proposed,
where adapter layers are inserted only after the ’Add & Norm’
step following the FFN layer to enhance the computational
efficiency. The adapters mentioned above follow a sequen-
tial design, placing adapter layers as bottlenecks within the
Transformer blocks. This approach may potentially reduce the
model’s parallelism and require a trade-off between inference
efficiency and accuracy. In contrast, [32] introduced a parallel
6
Transformer 
Module
Adapter
Transformer 
Module
(a) Serial Adapter (b) Parallel Adapter 
Transformer 
Module
(c) CoDA
all tokens all tokens top-k tokens all tokens
(d) Adapter Layer
Wdown
Wup
ReLU
Adapter Adapter
Fig. 5: Illustration of three representative adapter-based fine-tuning algorithms. Blue represents frozen, while yellow represents
trainable.
adapter (PA) approach as depicted in Figure 5 (b), which
Metadata: {}
--------------------------------------------------

Chunk 29:
Page Content: trainable.
adapter (PA) approach as depicted in Figure 5 (b), which
reorganizes the traditionally sequential adapter layers into a
parallel side-network that runs alongside each Transformer
sublayer. Similarly, CIAT [33], CoDA [34] and KronA [78]
also adopts a parallel adapter design. Except for the parallel
design, CoDA employs a sparse activation mechanism to
improve the inference efficiency as shown in Figure 5 (c).
Specifically, CoDA uses a soft top- k selection process that
identifies k important tokens in each layer, which will be
processed by both the frozen pre-trained Transformer layer and
the adapter branch to maintain model accuracy. In contrast,
those unimportant tokens are only processed by the adapter
branch while skipping the heavy pre-trained layer, therefore
optimizing for inference efficiency without compromising
overall performance.
To enhance the performance and generalization of adapters,
various studies have implemented multi-task learning strate-
Metadata: {}
--------------------------------------------------

Chunk 30:
Page Content: various studies have implemented multi-task learning strate-
gies, such as AdapterFusion [35], AdaMix [36], PHA [37],
AdapterSoup [38], MerA [39], and Hyperformer [40].
AdapterFusion keeps all pre-trained adapters in the model
and employs a fusion module to merge the multi-task in-
formation. Unlike AdapterFusion, MerA merges pretrained
adapters into a single one through optimal transport based
on weights and activations. This approach avoids introduc-
ing any additional trainable parameters, thereby enhancing
computational efficiency. Hyperformer stores the multi-task
information in a shared hypernetwork, which generates task
and layer-specific adapter parameters conditioned on task and
layer ID embeddings. Given a new task, only an additional
task embedding needs to be learned, therefore reducing the
number of trained parameters.
2) Soft Prompt: Alternatively, prompt tuning presents an
additional approach for refining the model to achieve improved
Metadata: {}
--------------------------------------------------

Chunk 31:
Page Content: additional approach for refining the model to achieve improved
performance through fine-tuning. Instead of optimizing dis-
crete token representations through in-context learning, there
is a prevailing belief that the continuous embedding space
of soft prompts inherently contains more information [103].
Drawing inspiration from this concept, researchers directly
prepend adjustable vectors, referred to as soft prompts, to the
start of the input sequence. This can be represented as follows:
Xplq “rsplq
1 , . . . ,splq
NS , xplq
1 , . . . ,xplq
NX s (8)
where Xplq is the sequence of input tokens for layer l,
including soft prompt tokens splq
i followed by the original input
tokens xplq
i . NS is the number of soft prompt tokens, and NX
is the number of original input tokens.
Prefix-tuning [41] introduces learnable vectors that are
prepended to keys k and values v across all Transformer layers.
To ensure stability during the optimization process, Prefix-
Metadata: {}
--------------------------------------------------

Chunk 32:
Page Content: To ensure stability during the optimization process, Prefix-
tuning adopts a reparameterization strategy, which utilizes
an MLP layer to generate these prefix vectors rather than
optimizing them directly. After fine-tuning, only the prefix
vectors are saved for inference. This technique has been
adapted and improved in several studies [42], [43], [44]. For
instance, p-tuning v2 [43] removes reparameterization and
expands its usage to broader model scales and NLP tasks.
APT (Adaptive Prefix Tuning) [44] enhances Prefix-tuning by
introducing an adaptive gate mechanism to control the prefix
importance in each layer. Concurrent work p-tuning [45]
and prompt-tuning [46] apply learnable vectors only at the
initial word embedding layer rather than all layers to enhance
training and inference efficiency. It’s important to highlight
that prompt-tuning demonstrates its effectiveness primarily in
the context of large models, specifically those with over 11
Metadata: {}
--------------------------------------------------

Chunk 33:
Page Content: the context of large models, specifically those with over 11
billion parameters [46]. Complementing this, Xprompt [47]
eliminates the negative prompt tokens through a hierarchi-
cally structured pruning, which closes the performance gap at
smaller model scales. [104] provides some theoretical analysis
towards prompt tuning, demonstrating its universality and
limitations in limited-depth Transformers. IDPG (Instance-
Dependent Prompt Generation) [48] improves prompt tuning
by generating prompts based on each input sentence with
a lightweight prompt generator. In a related approach, LPT
(Late Prompt Tuning) [49] also leverages a prompt generator
to obtain instance-aware prompt. Unlike previous work, LPT
adds these prompts only after an intermediate layer, rather than
at the initial or all layers. This strategic placement eliminates
the gradient calculation below the intermediate layer, thereby
significantly accelerating the training speed. Simultaneously,
Metadata: {}
--------------------------------------------------

Chunk 34:
Page Content: significantly accelerating the training speed. Simultaneously,
LPT can improve the overall performance due to the shorter
backpropagation path preserves more task-related information.
Inspired by LPT, SPT (Selective Prompt Tuning) [50] delves
deeper into the importance of prompt inserting strategies.
It introduces a learnable probabilistic gate in each layer to
determine whether to use the prompt propagated from the pre-
vious layer or inject a newly generated prompt. APrompt [51]
employs another prompt inserting strategy. In addition to input
prompts inserted at the beginning of the input sequence for
each Transformer layer, APrompt also prepends additional
learnable prompts to the respective query, key, and value
7
V K Q
⊙
lk
lv lff
⊙
softmax
Wdown
𝛔
⊙
Wup
(a) (IA) 3
⊙ ⊕
Operation 1
Operation 2
(b) SSF
scale shift
Fig. 6: Illustration of (IA) 3 and SSF. Blue represents frozen,
while yellow represents trainable.
matrices in the self-attention blocks to learn new attention
Metadata: {}
--------------------------------------------------

Chunk 35:
Page Content: while yellow represents trainable.
matrices in the self-attention blocks to learn new attention
patterns. Besides, APrompt incorporates the learning of a task-
specific head.
The concept of soft prompts has been employed for various
downstream tasks [105], [106], although their training can
be prone to instability and slow convergence. To address
this, SPoT [52] uses a source prompt learned from one or
multiple tasks to initialize prompts for new tasks. Similarly,
the transfer of soft prompts from one task to initialize another
is proposed in TPT (transferable prompt tuning) [53], which
demonstrates that a better prompt initialization results in a
large training convergence speedup.InfoPrompt [54] develops
two mutual information-based loss functions, i.e., head loss
and representation loss , to find better prompt initialization
and learn sufficient task-relevant information, thereby also
expediting convergence. PTP [55] delves into the root causes
Metadata: {}
--------------------------------------------------

Chunk 36:
Page Content: expediting convergence. PTP [55] delves into the root causes
of training instability. It identifies the steep nature of the loss
landscape in conventional prompt tuning, where minor varia-
tions in input data can lead to significant loss fluctuations. To
mitigate this, PTP introduces perturbation-based regularizers
to smooth the loss landscape and consequently stabilize the
training process. DePT [58] decomposes the soft prompt into
a shorter soft prompt with a pair of low-rank matrices, which
are optimized with two distinct learning rates. This strategy
not only improves performance but also enhances training and
inference efficiency. SMoP (Sparse Mixture-of-Prompts) [57]
reduce the training and inference cost by utilizing short soft
prompts. During training, multiple short soft prompts are
trained, each tailored to specific subsets of the dataset. During
inference, SMoP integrates a gating mechanism that routes
each input instance to an appropriate short prompt. This
Metadata: {}
--------------------------------------------------

Chunk 37:
Page Content: each input instance to an appropriate short prompt. This
technique not only increases efficiency in both training and
inference stages but also retains performance comparable to
those achieved with longer soft prompts. To further cut down
the number of soft prompt parameters, IPT (Intrinsic Prompt
Tuning) [56] identifies an intrinsic task subspace by training
an auto-encoder on multiple tasks. Tuning on new tasks then
requires adjusting only a few parameters within this subspace,
significantly reducing the number of training parameters.
3) Other Additive Methods: Apart from the methods men-
tioned above, there appear other approaches that strategi-
cally incorporate additional parameters during the fine-tuning
process. For example, (IA)3 [59] introduces three learnable
rescaling vectors: lk P Rdk , lv P Rdv , and lff P Rdff ,
to rescale the key, value, and FFN activations, respectively,
as depicted in Figure 6 (a). The operations within the self-
Metadata: {}
--------------------------------------------------

Chunk 38:
Page Content: as depicted in Figure 6 (a). The operations within the self-
attention block can be described as follows:
SApxq“ Softmax pQplk dKT q?dhead
qpplv dV q. (9)
In FFN, the rescaling can be denoted as:
F F NTransfomer pxq“ Wupplff dσpWdownxqq, (10)
where dis Hadamard product. Furthermore, the scale vectors
lk and lv can be seamlessly integrated into the weight matrices
of AQ and AW . This integration effectively eliminates the ex-
tra computational costs during inference. A similar technique
SSF [61] also performs linear transformation to the model
activations, as illustrated in Figure 6 (b). Specifically, after
each operation (i.e., MSA, FFN, and layer normalization) in
the pre-trained model, an SSF-ADA layer is injected, which
performs scaling and shifting to the features generated from
the operation. During fine-tuning, only those SSF-ADA layers
can be updated, while during inference, similar to (IA) 3, these
SSF-ADA layers can be merged into model weights, so no ad-
Metadata: {}
--------------------------------------------------

Chunk 39:
Page Content: SSF-ADA layers can be merged into model weights, so no ad-
ditional inference overhead would be incurred. IPA (Inference-
Time Policy Adapters) [62] offers a novel approach to align
LLMs, such as GPT-4, with user-specific requirements without
modifying the base model’s parameters. This is particularly
significant when dealing with models whose parameters are
extremely large and often not directly accessible. IPA achieves
this by combining (through multiplication and normalization)
the output distribution of a base LLM (base policy) with that
of a smaller-sized model (adapter policy) during the decoding
phase. During training, the policy adapter’s parameters are
fine-tuned using reinforcement learning, while the base pol-
icy’s parameters remain fixed. During inference, IPA decodes
with the combined distribution of the base model and the
trained policy adapter, tailoring it to fulfill specific user-defined
criteria.
B. Selective PEFT
Rather than additive PEFT, which increases the model
Metadata: {}
--------------------------------------------------

Chunk 40:
Page Content: criteria.
B. Selective PEFT
Rather than additive PEFT, which increases the model
complexity by adding more parameters, selective PEFT fine-
tunes a subset of the existing parameters to enhance model
performance over downstream tasks, as depicted in Figure 4
(b).
Specifically, given a model with parameters θ “
tθ1, θ2, ..., θnu where each θi denotes an individual model
parameter and n represents the total count of these parameters,
the process of selective PEFT is represented by applying a
binary mask M “tm1, m2, ..., mnuto these parameters. Each
mi in M is either 0 or 1, indicating whether the corresponding
parameter θi is selected (1) or not selected (0) for fine-tuning.
The updated parameter set θ1 after fine-tuning is given by:
θ1
i “θi ´η ¨mi ¨ BL
Bθi
(11)
where η represents the learning rate, and BL
Bθi
is the gradient
of the loss function with respect to the parameter θi. In this
formulation, only the parameters that are selected (i.e., mi “
Metadata: {}
--------------------------------------------------

Chunk 41:
Page Content: formulation, only the parameters that are selected (i.e., mi “
1) are updated during backpropagation.
8
 
(a) Unstructural 
Masking
(b) Structural 
Masking
Frozen Learnable
Fig. 7: Illustration of two parameter masking methods.
Diff pruning [63] is a representative work that applies
a learnable binary mask to the model weights during fine-
tuning. To achieve parameter efficiency, the mask is reg-
ularized by a differentiable approximation of the L0-norm
penalty. PaFi [65] simply select model parameters with the
smallest absolute magnitude as trainable. FishMask [66] de-
termines parameter importance using the approximate Fisher
information. It then selects the top k parameters based on this
information to form the mask M. Similarly, Fish-Dip [67]
also uses Fisher information to calculate M, but the mask
will be re-calculated dynamically in each train period. LT-
SFT [68] introduces another technique to determine parameter
importance inspired by the Lottery Ticket Hypothesis [107],
Metadata: {}
--------------------------------------------------

Chunk 42:
Page Content: importance inspired by the Lottery Ticket Hypothesis [107],
[108], where the subset of parameters that change the most
during an initial fine-tuning stage is selected to form the mask
M. SAM [69] proposes a second-order approximation method,
which approximates the original problem with an analytically
solvable optimization function, to help decide the parameter
mask. Child-tuning [70] proposes two approaches to select a
child network during each training iteration, where only the
parameters within this child network can be updated.
However, the above unstructured parameter masking results
in an uneven distribution of non-zero masks and diminished
hardware efficiency when implementing PEFT. As shown in
Figure 7, the structured mask organizes parameter masking
in regular patterns, unlike unstructured ones that apply it
randomly, thus enhancing computational and hardware effi-
ciency during training. Therefore, various structured selective
Metadata: {}
--------------------------------------------------

Chunk 43:
Page Content: ciency during training. Therefore, various structured selective
PEFT techniques have undergone extensive investigation. Diff
pruning proposes a structured pruning strategy by partitioning
the weight parameters into local groups and strategically elim-
inating them together. Similarly, FAR [71] fine-tunes BERT
models by grouping weights of the FFN in Transformer blocks
into nodes, then ranking and selecting the learner nodes using
L1 norm. To further reduce the memory access frequency,
they also reconfigure the FFN by grouping the learner nodes.
Bitfit [72] is proposed to only fine-tune the bias parameters
of each DNN layer, and achieves competitive results for small
models. However, this method fails to handle large models.
[64] applies NAS to Bitfit, where S-BitFit keeps the structural
nature in Bitfit that restricts NAS algorithm must choose
whether δb “ 0 or not for each bias module. Similar to
Bitfit that fine-tunes a specific module in Transformer, Xattn
Metadata: {}
--------------------------------------------------

Chunk 44:
Page Content: Bitfit that fine-tunes a specific module in Transformer, Xattn
Tuning [73] fine-tunes only the cross-attention layers. SPT
(sensitivity-aware visual parameter-efficient fine-tuning) [74]
first identifies the sensitive parameters measured by the loss
reduction when being tuned. This sensitivity is calculated
using a first-order Taylor expansion, derived from a single
forward and backward pass before fine-tuning in one shot.
Next, SPT finds the weight matrices whose number of sensitive
parameters exceeds a pre-defined threshold and then applies
a selected PEFT technique (e.g., LoRA and Adapter) to these
targeted weights to achieve structural tuning.
C. Reparameterized PEFT
Reparameterization stands for equivalently transforming a
model’s architecture from one to another via transforming
its parameters. In the context of PEFT, this often means
constructing a low-rank parameterization to achieve the goal of
parameter efficiency during training. For inference, the model
Metadata: {}
--------------------------------------------------

Chunk 45:
Page Content: parameter efficiency during training. For inference, the model
can be converted to its original weight parameterization, en-
suring unchanged inference speed. This procedure is depicted
in Figure 4 (c).
Earlier research studies [75] have shown that common
pre-trained models exhibit an exceptionally low intrinsic di-
mensionality. In other words, it is possible to find a low-
dimensional reparameterization that is effective for fine-tuning
as the entire parameter space. Intrinsic SAID [75] is the pi-
oneering work in investigating the intrinsic dimension feature
during the fine-tuning of LLMs. However, the most widely
recognized reparameterization technique is LoRA (Low-Rank
Adaptation) [76], [109], as shown in Figure 8 (a). For a given
pre-trained weight matrix W0 PRdˆk, LoRA introduces two
trainable weight matrices, Wup P Rdˆr and Wdown P Rrˆk
where the rank r ! minpd, kq, operating in parallel to W0.
Let hin represent the input. Under normal conditions, the
Metadata: {}
--------------------------------------------------

Chunk 46:
Page Content: Let hin represent the input. Under normal conditions, the
output through W0 is hout “W0hin. Instead, LoRA modifies
this output by introducing an incremental update ∆W that
encapsulates task-specific knowledge:
hout “W0hin `α
r ∆W hin “W0hin `α
r WupWdownhin,
(12)
where α denotes a scaling factor. At the onset of training,
Wdown is initialized using a random Gaussian distribution,
while Wup is initialized to zero, ensuring that ∆W initially
holds a value of zero. LoRA is straightforward to implement
and has been evaluated on models with up to 175 billion
parameters. Fig 8 (c) used a single decoder as an example,
the frozen and learnable components are highlighted in grey
and red, respectively. Once fine-tuning is complete, LoRA’s
adaptive weights seamlessly integrate with the pre-trained
backbone weights. This integration ensures that LoRA main-
tains the model’s efficiency, adding no extra burden during
inference.
In LoRA training, selecting an appropriate rank has always
Metadata: {}
--------------------------------------------------

Chunk 47:
Page Content: inference.
In LoRA training, selecting an appropriate rank has always
been a challenging issue. To address this, DyLoRA [82], as
depicted in Figure 8 (b), trains the LoRA module on a range of
ranks within a predefined training budget, rather than adhering
to a single, fixed rank. Specifically, for a given rank range R “
trmin, rmin`1, . . . , rmaxu, DyLoRA dynamically chooses a rank
r PR at each iteration of the training process. Consequently,
the matrices Wdown and Wup are tailored for the selected rank
r, resulting in truncated versions WdownÓr “ Wdownr1 : r,:s
and WupÓr “ Wupr:, 1 : rs, and the subsequent forward
and backward pass during this iteration will be restricted
9
Pre-trained
Weights
W0 ∊ Rd×k Wdown ∊ Rr×k
Wup ∊ Rd×r
(a) LoRA
× d
d
rmax
r
r
rmax
Wdown↓r
Wup↓r
Wup Wdown
×
(b) DyLoRA
Pre-trained
Weights
Decompose
W0 ∊ Rd×k
Pre-trained
Weights
                 
  
Magnitude
Direction
m∊ R1×k
1/॥V+ΔV॥c
×
V ∊ Rd×k
Wup ∊ Rd×r
Wdown ∊ Rr×k
×
(c) DoRA
Metadata: {}
--------------------------------------------------

Chunk 48:
Page Content: Magnitude
Direction
m∊ R1×k
1/॥V+ΔV॥c
×
V ∊ Rd×k
Wup ∊ Rd×r
Wdown ∊ Rr×k
×
(c) DoRA
Fig. 8: Illustration of three representative reparameterized PEFT algorithms. Blue represents frozen, while yellow represents
trainable.
on WdownÓr and WupÓr instead of Wdown and Wup. With
this dynamic and search-free approach, DyLoRA significantly
reduces the training time required to find an optimal and fixed
LoRA rank for specific tasks. AdaLoRA [83] reformulates
the ∆W with a singular value decomposition (SVD), denoted
as ∆W “ PΛQ, where P P Rdˆr and Q P Rrˆk are
orthometric, Λ is a diagonal matrix containing singular values
tλiu1ďiďr. All three weight matrices are made learnable.
During training, the singular values are pruned iteratively
based on their importance scores, which are constructed from
the moving average of the magnitude of the gradient-weight
product. To ensure the orthogonality between P and Q, i.e.,
PT P “QQT “I, an additional regularizer term is included
in the loss:
RpP, Qq“
Metadata: {}
--------------------------------------------------

Chunk 49:
Page Content: PT P “QQT “I, an additional regularizer term is included
in the loss:
RpP, Qq“
››PT P ´I
››2
F `
››QQT ´I
››2
F . (13)
This adaptive approach enables the model to dynamically ad-
just the rank within each LoRA module, effectively managing
its parameter counts based on the significance of the weight
matrices. However, according to SoRA [84], the importance
scores used in AdaLoRA are heuristically constructed, which
lacks rigorous theoretical motivation. Additionally, both mov-
ing average operation and calculation of Eq. 13 introduce
extra computation costs during training. To address this, SoRA
eliminates the orthogonality premise of P and Q. Instead, a
gating unit g PRr between Wup and Wdown is directly applied
and optimized:
hout “Wuppg dpWdownhinqq, (14)
where dis Hadamard product. The gate g is updated using a
variation of proximal gradient iteration for l1 loss [110], [111],
which has a clear mathematical meaning and does not need
Metadata: {}
--------------------------------------------------

Chunk 50:
Page Content: which has a clear mathematical meaning and does not need
the heuristic premise. After training, the zeroed-out gate units
are pruned by removing the corresponding columns and rows
in Wdown and Wup.
Several subsequent studies have aimed to improve LoRA’s
performance in various aspects. For instance, Laplace-
LoRA [87] notices that fine-tuned LLMs often exhibit over-
confidence. To enhance the calibration of fine-tuned LLMs,
Laplace-LoRA utilizes a Bayesian approach, specifically a
post-hoc Laplace approximation [112], [113], to the posterior
over the LoRA parameters. LoRA Dropout [88] introduces
random noises to the learnable low-rank matrices and in-
creases parameter sparsity to reduce the risk of overfitting.
LoRA+ [90] proposes to set different learning rates for the
LoRA matrices Wdown and Wup, such that ηup “ληdown with
λ ą1 fixed and tune ηdown. MoSLoRA (Mixture-of-Subspaces
LoRA) [91] decomposes LoRA into subspaces via structural
Metadata: {}
--------------------------------------------------

Chunk 51:
Page Content: LoRA) [91] decomposes LoRA into subspaces via structural
reparameterization, then employs a learnable mixer, trained
jointly with the original LoRA weights, to fuse the subspaces.
Similarly to LoRA, MoSLoRA can also be merged into the
original weights.
Thanks to the modular design of LoRA, many studies
incorporate multiple LoRA modules in their frameworks to
enhance performance. For example, LoRAHub aggregates
various LoRA modules trained on different tasks. Given
a handful of examples from a new task, LoRAHub can
autonomously compose compatible LoRA modules without
human intervention via a gradient-free method Shiwa [114].
MOELoRA employs a Mixture-of-Experts (MOE) approach
to train LoRA in a multi-task setting, resulting in multiple
expert LoRA modules. To retrieve parameters for certain tasks,
MOELoRA utilizes a task-motivated gate function that assigns
contribution weights to each expert based on the task ID, and
the final parameters are calculated through a weighted sum of
Metadata: {}
--------------------------------------------------

Chunk 52:
Page Content: the final parameters are calculated through a weighted sum of
all experts.
In addition to LoRA, several other reparameterization tech-
niques are emerging with significant potential. For instance,
Compacter [77] introduces a light-weight adapter modules
by parameterizing the Wdown and Wup as W “řn
i“1 Ai bBi,
where Ai PRnˆn, Bi PR
r
n ˆd
n , and bdenotes the Kronecker
product. They further decrease the parameter count by desig-
nating Ai as shared parameters and reparameterizing Bi using
the product of two low-rank matrices, effectively reducing the
parameter complexity from Oprdqto Opr`dq. Related studies,
such as KronA [78] and KAdaptation [79], also employ the
Kronecker product to reparameterize adapter weights, aiming
to achieve parameter reduction. HiWi [65] proposes an adapter
fine-tuning method that applies an adapter directly to pre-
trained parameters instead of hidden representations as:
W1 “W `σpW WdownqWup, (15)
where W denotes the weights or biases within the Transformer
Metadata: {}
--------------------------------------------------

Chunk 53:
Page Content: W1 “W `σpW WdownqWup, (15)
where W denotes the weights or biases within the Transformer
block’s feed-forward layer. Notably, during inference, this
method computes W1 in advance, ensuring that the model’s
10
inference latency remains on par with that of traditional
full fine-tuning. VeRA (Vector-based Random Matrix Adapta-
tion) [80] employs a single pair of frozen low-rank matrices
Wup and Wdown that are shared across all layers, and adapts
these matrices by learning small, trainable scaling vectors
represented as b and d (formally denoted by diagonal matrices
Λb and Λd). Specifically, the reparameterization is given by:
hout “W0hin `ΛbWupΛdWdownhin, (16)
where both Wup and Wdown are initialized using a random
Gaussian distribution. Similar to LoRA, the scaling vector
b is initialized to zeros to ensure that the weight matrix is
unaffected during the first forward pass. This method signif-
icantly reduces the number of trainable parameters compared
Metadata: {}
--------------------------------------------------

Chunk 54:
Page Content: icantly reduces the number of trainable parameters compared
to LoRA yet maintains the same performance, enabling the
fine-tuning of larger models on a single GPU. DoRA (Weight-
Decomposed Low-Rank Adaptation) [81] presents a novel
approach as illustrated in Figure 8 (c) by decomposing model
weights W0 PRdˆk into magnitude and direction as follows:
W0 “m V
}V }c
“}W0}c
W0
}W0}c
, (17)
where m P R1ˆk is the magnitude vector, V P Rdˆk is
the directional matrix, with }¨} c being the vector-wise norm
of a matrix across each column. Subsequently, DoRA adopts
a unique fine-tuning strategy for m and V . While both are
tunable, only V undergoes LoRA reparameterization, defined
as:
W1 “m V `∆V
}V `∆V }c
“m
W0 `WupWdown
}W0 `WupWdown}c
, (18)
where ∆V is the incremental directional update learned by
LoRA, and the underlined parameters denote the trainable
parameters. Through this methodology, DoRA consistently
outperforms LoRA across various tasks and models, demon-
strating its superiority.
Metadata: {}
--------------------------------------------------

Chunk 55:
Page Content: outperforms LoRA across various tasks and models, demon-
strating its superiority.
D. Hybrid PEFT
The efficacy of various PEFT methods can significantly
differ across different tasks. As a result, numerous studies aim
to either combine the advantages of diverse PEFT approaches
or seek to establish a unified perspective by analyzing the
similarities among these methods. For instance, UniPELT [97]
integrates LoRA, prefix-tuning, and adapters into each Trans-
former block. To control which PEFT submodules should
be activated, they also introduce a gating mechanism. This
mechanism consists of three small FFNs that each produce
a scalar value G P p0, 1q, which is then applied to the
LoRA, prefix, and adapter matrices, respectively. Across var-
ious setups, UniPELT has consistently shown improvements
in accuracy ranging from 1% to 4%. S4 [98] explores design
spaces for several PEFT methods (i.e., Adapter (A), Prefix
(P), BitFit (B), and LoRA (L)) to uncover underlying design
Metadata: {}
--------------------------------------------------

Chunk 56:
Page Content: (P), BitFit (B), and LoRA (L)) to uncover underlying design
patterns. After a series of experiments, their findings include:
(1) Applying the spindle grouping partitioning for Transformer
layers, which results in four layer groups Gi for i Pt1 . . .4u.
Layers in one group have similar behaviors together, which
means should apply similar PEFT strategies. (2) Allocating
the number of trainable parameters to layers uniformly. (3)
Tuning all the groups. (4) Assigning different PEFT strategies
in different groups. The resulting design space that has the
best performance is:
G1 : pA, Lq, G2 : pA, Pq, G3 : pA, P, Bq, G4 : pP, B, Lq
MAM Adapter [32] explores the intrinsic similarity between
three additive PEFT methods: adapters, prefix-tuning, and
LoRA, which leads to the development of three variants:
Parallel Adapter, which places adapter layers alongside spe-
cific layers (SA or FFN) instead of after them; Multi-head
Parallel Adapter , which divides the parallel adapter into
Metadata: {}
--------------------------------------------------

Chunk 57:
Page Content: Parallel Adapter , which divides the parallel adapter into
multiple heads, each affecting the head attention output in
SA; and Scaled Parallel Adapter , which adds a scaling term
after the parallel adapter layer, similar to LoRA. Extensive
experimentation revealed that the most effective configura-
tion involves using prefix-tuning in the SA layer and the
scaled parallel adapter in the FFN layer, which is called the
MAM Adapter. LLM-Adapters [101] builds an easy-to-use
framework that incorporates various PEFT techniques into
LLMs. Through comprehensive benchmarking across multiple
datasets, the study reveals several key insights: (1) The most
effective locations for series adapters, parallel adapters, and
LoRA are after the MLP layers, alongside the MLP layers, and
simultaneously following the Attention layers and MLP layers,
respectively. (2) Smaller LLMs utilizing PEFT can achieve
competitive or even superior results on certain tasks when
Metadata: {}
--------------------------------------------------

Chunk 58:
Page Content: competitive or even superior results on certain tasks when
compared to their larger counterparts. (3) With appropriate
in-distribution fine-tuning data, smaller models are capable of
surpassing larger models in task-specific performance.
Several studies leverage neural architecture search (NAS)
to find better PEFT combination approaches. For example,
NOAH [99] discovers that different PEFT configurations are
specifically tailored for different tasks. To address this issue,
NOAH employs NAS to identify the most effective PEFT con-
figurations for each dataset. Specifically, NOAH’s searching
space encompasses three PEFT methods: Adapter, LoRA, and
Visual Prompt Tuning (VPT). It utilizes AutoFormer [115], a
one-shot NAS algorithm, for the efficient discovery of optimal
prompt modules. In a related vein, AUTOPEFT [100] first
establishes a searching space that includes serial adapters,
parallel adapters, and prefix tuning. After that, they propose
Metadata: {}
--------------------------------------------------

Chunk 59:
Page Content: parallel adapters, and prefix tuning. After that, they propose
an effective NAS method based on a high-dimensional multi-
dimensional Bayesian optimisation [116]. Both NOAH and
AUTOPEFT demonstrate the capability of NAS in enhancing
PEFT configurations across a variety of tasks.
IV. E FFICIENT PEFT DESIGN
Processing latency and peak memory overhead are pivotal
factors to consider from a computational standpoint. This
section introduces a key characteristic in LLMs aimed at
balancing between latency and memory usage (Section IV-A).
Following this, we explore strategies for developing efficient
PEFT methods to address computational challenges, including
PEFT pruning (Section IV-B), PEFT quantization (Sec-
tion IV-C), and memory-efficient PEFT techniques (Sec-
tion IV-D), each designed to enhance model performance
11
Efficient PEFT Design
PEFT Pruning AdapterDrop [117], SparseAdapter [118], SPLoRA [119], LoRAPruning [120], ProPETL [121]
PEFT
Quantization
Metadata: {}
--------------------------------------------------

Chunk 60:
Page Content: PEFT
Quantization
BI-Adapter [122], PEQA [123], QLoRA [124], LoftQ [125], LQ-LoRA [126], QA-LoRA [127], INT2.1 [128], QDyLoRA [129],
BitDelta [130]
Memory-efficient
PEFT
Side-Tuning [131], LST [132], Res-Tuning [133], MEFT [134], LoRA-FA [135], HyperTuning [136], PEFT Plug-in [137],
MeZO [138], GaLore [139]
Fig. 9: Taxonomy of Efficient PEFT Design.
while minimizing resource consumption. It is noteworthy that
quantization inherently addresses memory overhead concerns.
However, given its distinct characteristics, we address these
quantization methods separately rather than incorporating
them under the memory-efficient PEFT section.
A. KV-cache Management for PEFT Efficiency
The core of the LLMs model lies in an auto-regressive
Transformer model. When we consider the auto-regression
characteristic, it becomes a major challenge in designing an
inference system, because every time a new token is generated,
the entire LLM model has to transfer all the weights from
Metadata: {}
--------------------------------------------------

Chunk 61:
Page Content: the entire LLM model has to transfer all the weights from
different memories to the memory of the graphics processor,
which is very unfriendly to single-user task scheduling or
multi-user workload balance. The challenging part of serving
the auto-regressive paradigm is that all previous sequences
have to be cached and saved for the next proceeding iteration;
the cached activation generated from the previous sequences
is stored as the Key-Value Cache (KV-cache). To effectively
manage these challenges, S-LoRA [140] employs a Unified
Paging mechanism within a unified memory pool that dynam-
ically allocates and manages memory in a paged fashion. This
sophisticated approach minimizes memory fragmentation and
enhances the efficiency of KV-cache storage by allowing for
flexible and efficient memory access patterns. These pages are
managed such that the KV-cache associated with each adapter
is segmented into manageable blocks, streamlining access and
Metadata: {}
--------------------------------------------------

Chunk 62:
Page Content: is segmented into manageable blocks, streamlining access and
reducing the overhead associated with variable cache sizes. By
dynamically adjusting to different KV-cache requirements, S-
LoRA maintains high throughput and performance, ensuring
that the system remains responsive and efficient even as it
scales to serve thousands of adapters simultaneously. This
efficient handling of KV-cache is crucial for supporting the
auto-regressive nature of LLMs in high-demand environments,
optimizing both single-user and multi-user workload balanc-
ing.
B. Pruning Strategies for PEFT
The inclusion of pruning can substantially enhance the
efficiency of PEFT methods. In particular,AdapterDrop [117]
explores the removal of adapters from lower transformer
layers and multi-task adapters in AdapterFusion [35], which
shows that the pruning can improve the training and in-
ference efficiency with minimal decrease in performance.
SparseAdapter [118] investigates different pruning methods
Metadata: {}
--------------------------------------------------

Chunk 63:
Page Content: SparseAdapter [118] investigates different pruning methods
and finds that high sparsity ratio ( 80%) can outperform stan-
dard adapters. Additionally, the Large-Sparse configuration,
which increases the bottleneck dimension while maintaining
a constant parameter budget (e.g., doubling dimensions with
a 50% sparsity), substantially enhances the model’s capacity,
resulting in improved performance. SPLoRA [119] adopts
channel-based pruning to the LoRA weights Wdown and Wup.
This pruning affects not only the source weights W0, but
also the LoRA parameters Wup and Wdown. Similarly, Lo-
RAPruning [120] adopts structured pruning not only to the
pretrained model weights but also to the LoRA weights.
In contrast to unstructured LoRA pruning methods, which
primarily focus on sparsifying model weights while leaving
LoRA weights dense, thus making weight merging challenging
to achieve, LoRAPruning enables the weights to be merged
easily. Additionally, this work also introduces a novel criterion
Metadata: {}
--------------------------------------------------

Chunk 64:
Page Content: easily. Additionally, this work also introduces a novel criterion
that utilizes LoRA’s gradients as an approximation of the
gradients for the pre-trained weights, enabling the estimation
of weight importance. ProPETL [121] constructs a single
shared prototype (e.g., adapter, prefix, or LoRA) across layers
and tasks. In addition, ProPETL learns binary masks to prune
different sub-networks in different layers and tasks. As a result,
the parameters can be reused across layers and tasks, largely
increasing the parameter efficiency.
C. Quantization Strategies for PEFT
Quantization serves as another popular technique for im-
proving computational efficiency and reducing memory us-
age. For example, by investigating the loss landscape of
adapters, BI-Adapter [122] finds that adapters are resistant
to noise in parameter space. Building on this insight, the
authors introduce a clustering-based quantization approach.
Remarkably, they demonstrate that a 1-bit quantization of
Metadata: {}
--------------------------------------------------

Chunk 65:
Page Content: Remarkably, they demonstrate that a 1-bit quantization of
adapters not only minimizes storage requirements but also
achieves superior performance among all precision settings.
PEQA (Parameter-Efficient and Quantization-aware Adapta-
tion) [123] uses a two-stage pipeline to achieve parameter-
efficient and quantization-aware fine-tuning. In the first stage,
the pre-trained FFN weight matrix W P Rnˆm is quantized
to W “ s ¨W, where s P Rnˆ1 represents per-channel
scales and W denotes the quantized weight. In the second
stage, W remains fixed, and fine-tuning is only conducted
on s. This approach not only ensures memory efficiency but
also facilitates parameter efficiency. QLoRA [124] proposes
several novel techniques, including a 4-bit NormalFloat, a
Double Quantization, and a Paged Optimizers, to backprop-
agate a 4-bit quantized pretrained language model into LoRA.
12
These techniques enable the fine-tuning for a 65B language
model on a single 48GB GPU while maintaining similar
Metadata: {}
--------------------------------------------------

Chunk 66:
Page Content: model on a single 48GB GPU while maintaining similar
performance to the full 16-bit fine-tuning. Similar to the
original implementation [76], QLoRA attaches the fixed zero-
initialized LoRA weights to the quantized pre-trained model
as the training start point. However, when applying the ex-
treme low-bit (e.g., 2-bit) quantization, the huge quantization
error can adversely impact the initialization of LoRA fine-
tuning, i.e., quantizationpW0q` WdownWup ‰ W0 where
Wdown “0, which will harm the fine-tuning performance as
shown in the work by [134]. To solve this, several quanti-
zation strategies are proposed to eliminate the quantization
error. For example, LoftQ (LoRA-Fine-Tuning-aware Quanti-
zation) [125] presents an innovative framework that provides
a superior initialization point of quantized backbone weights
and LoRA weights for subsequent LoRA fine-tuning. This
approach addresses the discrepancies caused by quantization
Metadata: {}
--------------------------------------------------

Chunk 67:
Page Content: approach addresses the discrepancies caused by quantization
through the optimization of a Frobenius norm objective during
network initialization, which takes both the LoRA weights
and the quantized pre-trained backbone into consideration.
LoftQ exhibits superior performance in 2-bit quantization over
QLoRA, as well as greater generalization for downstream
tasks. LQ-LoRA [126] uses an iterative algorithm inspired
by robust principal components analysis [141], [142] which
decomposes the weight W0 such that W0 « Q ` L1L2
to resolve the inaccuracy caused by the quantization error,
where Q is the quantized component which remains fixed
and L1L2 is the trainable low-rank component. Moreover, this
approach leverages integer linear programming to determine
a mixed quantization strategy, enabling dynamic quantization
configurations for each weight matrix while adhering to a
predetermined total bit rate limit. QA-LoRA [127] address
another limitation of QLoRA, which struggles to preserve its
Metadata: {}
--------------------------------------------------

Chunk 68:
Page Content: another limitation of QLoRA, which struggles to preserve its
quantized property post-fine-tuning. In QLoRA, the quantized
pre-trained weight (NF4) has to be recovered to FP16 to match
the LoRA weight precision (FP16) during weight merging.
Instead, QA-LoRA uses INT4 quantization and introduces
group-wise operators to enable quantization during the infer-
ence stage, therefore improving the efficiency and accuracy
compared with QLoRA. BitDelta [130] introduces a novel 1-
bit post-training quantization method that acts on the weight
delta between a fine-tuned model and its underlying pre-
trained model. Specifically, given the weight matrices Wfine
and Wbase from the fine-tuned and base models respectively,
the weight delta ∆ “Wfine ´Wbase is binarized as ˆ∆ “α d
Signp∆q. Here, α, a high-precision scalar, is initialized based
on the mean absolute delta value α “ 1
nm
ř
ij |Wij|, with
Signp¨q indicating the sign of ∆. BitDelta further calibrates
Metadata: {}
--------------------------------------------------

Chunk 69:
Page Content: nm
ř
ij |Wij|, with
Signp¨q indicating the sign of ∆. BitDelta further calibrates
the scaling factors via distillation on a compact calibration
dataset, while the binary matrices remain unchanged. This
approach notably streamlines the deployment of multiple fine-
tuned models on shared servers by utilizing a singular full-
precision base model alongside efficiently batched 1-bit deltas.
D. Memory-efficient PEFT Methods
Fine-tuning the full LLMs necessitates substantial training
memory owing to their considerable size. While most PEFT
methods primarily target parameter efficiency, they still in-
cur a significant memory overhead during training because
gradient computation and backpropagation are still necessary
for these methods. For example, prevalent PEFT techniques
such as adapters and LoRA can only reduce memory usage
to approximately 70% compared to full model fine-tuning ac-
cording to some literature [132], [137]. From a computational
Metadata: {}
--------------------------------------------------

Chunk 70:
Page Content: cording to some literature [132], [137]. From a computational
perspective, memory efficiency also remains a critical factor
that cannot be overlooked.
To improve memory efficiency, various techniques have
been developed to minimize the need for caching gradi-
ents for the entire LLM during fine-tuning, thereby reducing
memory usage. For example, both Side-Tuning [131] and
LST (Ladder-Side Tuning) [132] introduce a learnable net-
work branch parallel to the backbone model. By channeling
the backpropagation exclusively through this parallel branch,
it circumvents the need to store gradient information for
the main model’s weights, thus markedly reducing memory
requirements during training. Similarly, Res-Tuning [133]
disentangles the PEFT tuners (e.g., prompt tuning, adapter)
from the backbone model. On top of the disentanglement, a
memory-efficient fine-tuning framework named Res-Tuning-
Bypass is proposed, which generates a bypass network in
Metadata: {}
--------------------------------------------------

Chunk 71:
Page Content: Bypass is proposed, which generates a bypass network in
parallel with the backbone model by removing the data flow
from the decoupled tuners to the backbone. This eliminates the
requirement for gradient caching within the backbone model
during backpropagation. MEFT [134] (memory-efficient fine-
tuning) is an approach inspired by the reversible model [143].
During the training of a reversible model, intermediate ac-
tivations are not required to be cached in the forward pass.
During backpropagation, they can be recalculated from the
final output. To save the memory during fine-tuning, MEFT
investigates how to transform an LLM to its reversible counter-
parts without additional pre-training. A critical aspect of this
transformation is the careful initialization of newly introduced
parameters in the pre-trained models. MEFT demonstrates the
importance of parameter initialization and suggests that these
parameters must be initialized in a manner that preserves the
Metadata: {}
--------------------------------------------------

Chunk 72:
Page Content: parameters must be initialized in a manner that preserves the
pre-trained model’s starting point, ensuring that the fine-tuning
of the modified model achieves performance on par with full
fine-tuning methods. With this key consideration, MEFT intro-
duces three distinct methods, each significantly curtailing the
memory demands traditionally required for storing activations.
LoRA-FA [135] addresses a limitation about memory over-
head in LoRA fine-tuning. During training, LoRA modules
still require high activation memory consumption. This is be-
cause, during backpropagation, large input activations must be
stored during the forward pass to compute gradients. LoRA-FA
resolves this issue by freezing both the pre-trained weights W0
and the projection-down weights Wdown, and only updating the
projection-up weights Wup. Consequently, the input activation
hin no longer needs to be stored, as the intermediate activation
Wdownhin is adequate for gradient computation for Wup. Given
Metadata: {}
--------------------------------------------------

Chunk 73:
Page Content: Wdownhin is adequate for gradient computation for Wup. Given
that r !d, the memory requirement for activations in LoRA-
FA can be significantly reduced.
To further reduce memory usage during fine-tuning, some
methods attempt to circumvent backpropagation within LLMs
to address this issue. HyperTuning [136] employs a Hyper-
13
Model to generate PEFT parameters using only fewshot exam-
ples. This approach demonstrates results comparable to those
obtained through full model fine-tuning. PEFT Plug-in [137]
first trains PEFT modules on small language models, which
is more memory efficient compared to training on large ones.
Subsequently, the research introduces a suite of techniques for
seamlessly integrating these trained PEFT modules into LLMs
during inference. This strategy effectively circumvents the
necessity of gradient-based optimization directly on the larger
models, resulting in substantial memory savings. However, it is
Metadata: {}
--------------------------------------------------

Chunk 74:
Page Content: models, resulting in substantial memory savings. However, it is
important to note that both HyperModel and PEFT Plug-in still
require additional model training, and this training cost cannot
be entirely overlooked. MeZO [138] introduces a memory-
efficient zeroth-order (ZO) optimizer for LLMs. Unlike con-
ventional PEFT techniques, which rely on backpropagation to
compute gradients for updating model parameters, MeZO fine-
tunes LLMs through only forward passes. It accomplishes this
by employing a ZO gradient estimator to calculate the gradient.
Notably, MeZO implements an in-place solution for the classic
ZO gradient estimator, effectively mitigating memory con-
sumption during inference execution. This innovative approach
allows for efficient fine-tuning of LLMs containing 30 billion
parameters on a single GPU with 80GB of memory, all while
maintaining performance that is comparable to fine-tuning
using backpropagation. Furthermore, it can substantially de-
Metadata: {}
--------------------------------------------------

Chunk 75:
Page Content: using backpropagation. Furthermore, it can substantially de-
crease storage demands in comparison to the traditional PEFT
methods such as LoRA and Adapter.
V. PEFT FOR DNN S OF OTHER APPLICATIONS
In Section III, we outlined four categories of PEFT methods
along with their improvements. Nonetheless, our discussion
did not fully extend to the utilization or adaptation of PEFT
techniques beyond traditional architectures (e.g., LLMs) or
standard benchmarks (e.g., the GLUE dataset), where the ma-
jority of the discussed PEFT methods are applied. Therefore,
in this section, we will highlight and discuss several most
representative works that leverage PEFT strategies for various
downstream tasks. We do not aim to cover all PEFT applica-
tion scenarios in this section. Our objective is to showcase the
significant influence of PEFT within various research domains
and demonstrate how to optimize and tailor general-purpose
PEFT methods to achieve enhanced performance in specific
models or tasks.
Metadata: {}
--------------------------------------------------

Chunk 76:
Page Content: PEFT methods to achieve enhanced performance in specific
models or tasks.
Typically, fine-tuning happens when adapting a pre-trained
backbone model to specialized downstream tasks. To this end,
this section organizes the discussion around various model
architectures, which include: LLM, Vision Transformer (ViT),
Vision-Language Alignment Model (VLA), and Diffusion
model. Within each architectural category, the discussion is
further classified based on different downstream tasks.
A. PEFT for LLMs – Beyond the Basics
Instead of common tasks in NLP such as NLU and NLG,
PEFT techniques boast a wide array of applications across
diverse scenarios. PEFT has been successfully implemented in
commonsense question answering [144], [145], multi-level im-
plicit discourse relation recognition [146], out-of-distribution
detection [147], privacy protection [148], [149], federated
learning [150], and social biases mitigation [151]. In this
section, we pay more focus on three representative downstream
Metadata: {}
--------------------------------------------------

Chunk 77:
Page Content: section, we pay more focus on three representative downstream
tasks: visual instruction following, continual learning, and
context window extension.
1) Visual Instruct Following: Several studies, including
VL-BART [152], MiniGPT-4 [153], and LLaV A [154], have
successfully extended the capabilities of LLMs, initially de-
signed for pure text, to comprehend and generate responses to
visual inputs. These enhanced models, namely visual instruct-
following LLMs, can process both images and text to produce
textual responses, which can be benchmarked on tasks such as
image captioning [155], [156], [157], [158] and visual question
answering (VQA) [159], [160], [161]. However, these methods
fine-tune the entire LLM to learn the visual representations,
which can be inefficient in both time and memory. Therefore, it
is natural to apply PEFT techniques in the fine-tuning of visual
instruct-following LLMs. An earlier work VL-Adapter [162]
directly applies several PEFT methods (Adapter [31], Hy-
Metadata: {}
--------------------------------------------------

Chunk 78:
Page Content: directly applies several PEFT methods (Adapter [31], Hy-
performer [40] and Compacter [77]) on VL-BART [152]
then benchmarks them on several image-text and video-text
tasks. Results show that vanilla adapters are the best among
them, which can achieve performance on par with full fine-
tuning. However, considering the functionality gap between
the encoders and decoders in VL-BART, directly assigning
identical modular modifications will lead to suboptimal perfor-
mance. Therefore, VL-PET [163] selectively integrates PEFT
modules into different components of the encoder and decoder.
They also introduce a granularity-controlled mechanism for
finer-grained control.
To adapt the recently prevalent LLaMA model, LLaMA-
Adapter [164] prepends a set of learnable prompts (similar
to prefix tuning) to the input tokens in LLaMA’s higher trans-
former layers. To avoid the unstable fine-tuning with large loss
values at early training stages, instead of the randomly initial-
Metadata: {}
--------------------------------------------------

Chunk 79:
Page Content: values at early training stages, instead of the randomly initial-
ized weights of other PEFT methods, LLaMA-Adapter adopts
a zero-initialized attention mechanism, which learns a zero-
initialized gating factor to adaptively control the contribution
of adaptation prompts to the word tokens. This can maintain
the fine-tuning starting point the same as the original model
and progressively inject new knowledge into the model, where
a similar idea can be found in MEFT [134] and LoftQ [125]
discussed earlier. To represent visual information, LLaMA-
Adapter extracts multi-scale global image features using a
CLIP image encoder and then projects them to linguistic em-
bedding space. After that, the feature is element-wisely added
onto the adaptation prompts at all inserted transformer layers.
LLaMA-Adapter only introduces 1.2M learnable parameters
in LLaMA-7B and costs less than one hour for fine-tuning on
8 A100 GPUs. A following work LLaMA-Adapter V2 [165]
Metadata: {}
--------------------------------------------------

Chunk 80:
Page Content: 8 A100 GPUs. A following work LLaMA-Adapter V2 [165]
demonstrates that the simple multimodal fusion in LLaMA-
Adapter cannot generalize to more challenging open-ended
multimodal reasoning tasks, where the visual cues tend to
dominate the adaptation prompts than the language instruction
data. To address this, LLaMA-Adapter V2 decouples the learn-
ing of instruction-following ability (to generate long language
responses) and vision-language alignment to avoid interfer-
ence between visual and language fine-tuning. Specifically,
14
LLaMA-Adapter V2 sets disjoint parameter groups which
are respectively learned from image-text pairs and language
instruction data. The visual adaptation prompts are inserted in
the early stage of LLM, while the language adaptation prompts
remain at the higher transformer layers similar to the LLaMA-
Adapter. Additionally, LLaMA-Adapter V2 introduces more
learnable parameters and several expert systems (e.g., caption-
Metadata: {}
--------------------------------------------------

Chunk 81:
Page Content: learnable parameters and several expert systems (e.g., caption-
ing, detection, and OCR) to enhance multimodal performance.
LayerNorm Tuning [166] adjust only the weights of the
LayerNorm within each attention block. This straightforward
technique can achieve comparable or even better performance
than the finetuning, while offering about 10× more parameter
efficiency than LoRA.
2) Continual Learning: Continual Learning (CL) aims to
learn a sequence of new tasks over time within one single
model, which has broad application in scenarios such as
dialogue systems [167], information extraction systems [168],
and question answering systems [169]. The main challenge in
CL is catastrophic forgetting [170]. A popular practice, called
architecture-based methods, tackles the CL by maintaining
task-specific parameters in the model for each new task. There-
fore, it’s natural to leverage PEFT methods for CL tasks [171],
[172], [173], [174]. For example, AdapterCL [171] pa-
Metadata: {}
--------------------------------------------------

Chunk 82:
Page Content: [172], [173], [174]. For example, AdapterCL [171] pa-
rameterizes each new task using residual adapters. During
testing, since the task-id is not provided, AdapterCL uses
an entropy-based classifier to select which adapter to use
for accomplishing a specific task. CPT (Continual Prompt
Tuning) [172] trains a soft prompt for each task. Instead of
training soft prompts from scratch, CPT proposes a series
of techniques (continual prompt initialization, query fusion,
memory replay, and a memory-guided technique) to achieve
knowledge transfer from preceding and subsequent tasks.
O-LoRA (orthogonal low-rank adaptation) [175] employs a
strategy of learning distinct tasks within separate low-rank
vector subspaces that are kept orthogonal to each other in order
to minimize interference. This approach can effectively reduce
catastrophic forgetting during the acquisition of new tasks.
3) Context Window Extension: LLMs are typically trained
with a pre-defined context size. For example, LLaMA and
Metadata: {}
--------------------------------------------------

Chunk 83:
Page Content: with a pre-defined context size. For example, LLaMA and
LLaMA2 have pre-defined context sizes of 2048 and 4096
tokens, respectively. The positional encoding RoPE has weak
extrapolation properties [176], which means the performance
drops obviously given an input length exceeds the pre-defined
context length. To solve this, a naive solution is to fine-
tune a pre-trained LLM to a longer context. However, this
escalates computational costs quadratically with context size,
straining memory and processing resources. To address this,
LongLoRA [177] proposes to fine-tune a pre-trained LLM
using LoRA to enlarge the context size. To reduce the
perplexity gap between LoRA tuning and full fine-tuning,
LongLoRA also opens embedding and normalization layers
for training. In order to further improve training efficiency in
a long context scenario, LongLoRA further introduces a novel
shifted sparse attention (S 2-Attn) as an efficient substitute for
Metadata: {}
--------------------------------------------------

Chunk 84:
Page Content: shifted sparse attention (S 2-Attn) as an efficient substitute for
standard self-attention during training. A subsequent study
LongQLoRA [178] combines the advantages of LongLoRA
with QLoRA and Position Interpolation [10] to save GPU
memory. This work successfully extends the context length
of LLaMA2-13B from 4096 to 8192 on a single V100 with
32GB memory. LLoCO [179] introduces a pipeline that
learns contexts offline through the combination of context
compression and LoRA. The process begins by compressing
documents into compact contexts, then fine-tuning LLM us-
ing LoRA on the compacted context to improve the LLM’s
ability to accurately extract and utilize information from these
compressed representations. During model serving, a standard
RAG retriever selects both the compressed document and the
most relevant LoRA module, and applies them to the LLM
for inference. This approach effectively extends the context
window of a 4k token LLaMA2-7B model to handle up to
128k tokens.
Metadata: {}
--------------------------------------------------

Chunk 85:
Page Content: window of a 4k token LLaMA2-7B model to handle up to
128k tokens.
In addition to limited training-stage sequence length, real-
world system memory constraints introduce another critical
bottleneck to the context window. Specifically, the capacity
of the KV-cache is curtailed by available system memory. For
example, a 30B parameter LLM operating with an input length
of 1024 and a batch size of 128 might necessitate up to 180GB
for the KV-cache [180], thereby restricting the feasible size of
the context window. In response to this, some strategies have
resorted to quantizing the KV cache [181], [182], but quanti-
zation will certainly compromise performance. To effectively
counteract this issue without significant loss, GEAR [183]
presents a novel approach by employing a low-rank matrix
to capture the majority of coherent bases of quantization
error, complemented by a sparse matrix that addresses errors
from outlier entries, thus efficiently minimizing approximation
errors.
Metadata: {}
--------------------------------------------------

Chunk 86:
Page Content: from outlier entries, thus efficiently minimizing approximation
errors.
B. PEFT for ViTs
ViT [184] has emerged as a powerful backbone model in the
recent computer vision community. In the ViT model, images
are treated as sequences of fixed-size patches analogous to how
LLM uses discrete tokens. These patches undergo linear em-
bedding and then receive positional encodings. Subsequently,
they are processed through standard Transformer encoders.
The training of ViT can be supervised [184], [185] or self-
supervised [186], [187], and ViT can achieve superior perfor-
mance when training with more data and using larger model
size [188]. However, such scaling up inevitably escalates
training and storage costs. Therefore, similar to LLMs, PEFT
is widely implemented in various downstream tasks, such as
dense prediction [189], continual learning [190], [191], deep
metric learning [192]. Here, we focus on two typical tasks to
showcase the involvement of PEFT: image classification and
Metadata: {}
--------------------------------------------------

Chunk 87:
Page Content: showcase the involvement of PEFT: image classification and
video recognition.
1) Image Classification: Image classification on targeted
visual datasets is a very common demand and has extensive
applications, while pre-train then fine-tuning paradigm serves
as a widespread strategy. A variety of methods leverage PEFT
techniques to achieve efficient model tuning [193], [189],
[194], [195]. For instance, AdaptFormer [194] inserts adapter
modules in parallel to the FFN of the original ViT model for
visual recognition tasks. VPT (Visual Prompt Tuning) [193]
prepends a small amount of task-specific parameters into the
input sequence of each Transformer layer. When applying
15
ViT to downstream tasks, only these added parameters and
the classification head are set to trainable. [196] notices that
compared with supervised ViT, VPT often underperforms with
self-supervised ViT. Further analysis demonstrates that differ-
ent pre-trained methods and downstream tasks have varying
Metadata: {}
--------------------------------------------------

Chunk 88:
Page Content: ent pre-trained methods and downstream tasks have varying
degrees of dependency on transformer blocks at different lo-
cations. To tackle this issue, the research introduces adaptable
gates for ViT blocks. These gates dynamically modulate the
contribution of prompt tokens to ViT blocks, allowing for a
more targeted adaptation of the model to the task at hand.
2) Video Recognition: Several works consider the more
challenging adaptation problem that transfers ViT to down-
stream tasks that have a much larger domain gap. For example,
ST-Adapter (Spatio-Temporal Adapter) [197] and AIM [198]
both insert adapters layers into pre-trained ViT blocks. Their
primary goal is to model spatial-temporal information, thereby
enabling efficient adaptation of ViTs from image models
to video tasks. Notably, both methodologies have exhibited
performance that surpasses traditional full-model fine-tuning
approaches.
C. PEFT for VLAs
Vision-language alignment models (VLA), such as
Metadata: {}
--------------------------------------------------

Chunk 89:
Page Content: approaches.
C. PEFT for VLAs
Vision-language alignment models (VLA), such as
CLIP [199], ALIGN [200], DeCLIP [201], and FLA V A [202],
are designed to learn a good image and text features which can
be aligned within a unified representation space. Each VLA
typically consists of separate image and text encoders that
extract respective features. Contrastive learning is leveraged in
these models to effectively align the image and text features.
Fine-tuning is leveraged to improve the performance of VLA
in specific datasets or tasks, but fine-tuning the full model
is computationally intensive. For instance, fine-tuning CLIP
RN50x64 requires a batch size of 32,768 and 18 days of
training on 592 V100 GPUs [199]. Moreover, full fine-tuning
on smaller datasets often leads to catastrophic forgetting [170].
In response to these challenges, and drawing inspiration from
the success of PEFT techniques in NLP, a range of PEFT
strategies have been proposed and implemented in VLA
Metadata: {}
--------------------------------------------------

Chunk 90:
Page Content: strategies have been proposed and implemented in VLA
models, such as semantic segmentation [203], [204], [205],
point cloud understanding [206], [207], [208], [209], video
understanding [210], [211], [212], visual reasoning [213],
[214], temporal action detection [215], to name a few. This
section will focus on one common task that uses VLAs:
open-vocabulary image classification.
1) Open-vocabulary Image Classification: In open-
vocabulary image classification, earlier works design
class-specific prompts, e.g., a photo of a [CLASS] , for each
category, and rank images based on their similarity to these
textual descriptions. CoOp (Context Optimization) [216]
replaces the handcrafted text prompt with learnable vectors,
while keeping the entire VLA fixes during training. CoCoOp
(Conditional Context Optimization) [217] builds on this by
tackling CoOp’s limitations in generalizing to unseen classes.
It introduces a lightweight neural network that generates an
Metadata: {}
--------------------------------------------------

Chunk 91:
Page Content: It introduces a lightweight neural network that generates an
input-specific context token, dynamically adapting the prompt
based on each image, thereby enhancing generalizability,
but at the cost of increased computational demands due to
the instance-aware operation. ProGrad [218] addresses the
over-fitting risk in CoOp in a few-shot setting by regularizing
the soft prompt updates whose gradient is aligned to the
general knowledge only updates the prompt whose gradient is
aligned (or non-conflicting) to the general knowledge offered
by the original prompt. MaPLe [219] notes that existing
methods learn prompts either in the language or in the vision
branch of CLIP, which is not efficient in leveraging the
multimodal nature of VLAs. To address this, MaPLe proposes
branch-aware hierarchical prompts that simultaneously adapt
both language and vision branches, and achieves superior
performance. TPT (test-time prompt tuning) [220] studies
Metadata: {}
--------------------------------------------------

Chunk 92:
Page Content: performance. TPT (test-time prompt tuning) [220] studies
prompt tuning on the fly without additional training samples.
Specifically, during inference, TPT first augments the input
image into various views, which are then utilized to tune
the learnable prompts. The primary training objective is to
ensure the VLA can generate consistent responses when faced
with these differing views. A following work DiffTPT [221]
further enhances the data diversity of test samples through
diffusion models.
In another direction, several studies explore the usage of
adapters in VLA. For example, CLIP-Adapter [222] inte-
grates residual-style adapters after CLIP’s text and visual en-
coders. Therefore, unlike CoOp and CoCoOp, CLIP-Adapter
avoids the gradient backpropagation through CLIP’s encoders,
leading to reduced computational requirements in terms of
both training memory and time. Tip-Adapter [223] adopts
the same design with CLIP-Adapter. Different from CLIP-
Metadata: {}
--------------------------------------------------

Chunk 93:
Page Content: the same design with CLIP-Adapter. Different from CLIP-
Adapter, the weights of the adapter are obtained in a training-
free manner from a query-key cache model [224], [225]
constructed from few-shot supervisions in a non-parametric
manner. As a result, Tip-Adapter exhibits great efficiency
compared to CLIP-Adapter’s SGD training process.
D. PEFT for Diffusion Models
Diffusion models [226], [227] are a class of generative
models that learn to generate data by transforming random
noise into a structured output by a progressive denoising
process. During training, diffusion models learn to reverse
the noise added to training data using a denoising network,
while in inference, they start from noise, using a denois-
ing network to iteratively create data that mirrors the same
distribution as the training examples. Diffusion models have
various applications [228], [229], [230], [231], [232], while the
most notable is stable diffusion [233], which bridges the gap
Metadata: {}
--------------------------------------------------

Chunk 94:
Page Content: most notable is stable diffusion [233], which bridges the gap
between text and image with its robust capability to generate
coherent and contextually relevant images directly from textual
descriptions. Numerous studies leverage PEFT techniques to
adapt a pre-trained diffusion model for downstream tasks, in-
cluding accelerating sampling speed [234], [235], text-to-video
adaptation [236], [237], text-to-3D adaptation [238], etc. This
section mainly focuses on two scenarios: integrating additional
input modalities beyond mere text-based conditioning, and
customizing content generation based on pre-trained diffusion
model.
1) Additional Input Control: To incorporate additional in-
put modalities (e.g., layout, keypoints) while retaining the
16
extensive knowledge in the pre-trained model, GLIGEN
introduces a novel approach, which maintains the original
model’s weights intact and integrates new, trainable gated
Transformer layers [239] that take in the new grounding
Metadata: {}
--------------------------------------------------

Chunk 95:
Page Content: Transformer layers [239] that take in the new grounding
input. The resulting model can not only accurately repre-
sent the grounding conditions but also produce high-quality
images. Remarkably, the model can also generalize well to
unseen objects during inference. ControlNet [240] fine-tunes
a trainable copy of the encoding layers from Stable Diffusion
while locking its pre-trained parameter weights. The fixed
original model and the trainable copy are bridged through zero
convolution layers. These layers, starting with zero-initialized
weights, are designed to progressively adapt during training,
ensuring that harmful noise does not affect the pre-trained
features of Stable Diffusion at the beginning of training. This
refined model is capable of conditioning on a variety of inputs
such as Canny edges, Hough lines, user scribbles, human key
points, segmentation maps, shape normals, depths, etc. Con-
cept Sliders [241] introduces a plug-and-play LoRA adaptors
Metadata: {}
--------------------------------------------------

Chunk 96:
Page Content: cept Sliders [241] introduces a plug-and-play LoRA adaptors
to allow precise editing of concepts (e.g., age, smiling) within
a diffusion model. T2I-Adapter [242] introduces a lightweight
adapter model designed to align external control signals with
the internal knowledge of text-to-image diffusion models. This
adapter enables precise manipulation through structural control
(e.g., sketch, depth map, semantic segmentation map, and
keypose), color control (e.g., hue and color distribution), and
integrating various controls by composing multiple adapters.
2) Customized Generation: The effectiveness of text-to-
image diffusion models is limited by the user’s ability to
articulate the desired target through text descriptions. For
instance, it is difficult to describe the precise features of an
innovative toy car which is not encountered during large-scale
model training. Consequently, the objective of customized
generation is to enable the model to grasp new concepts from a
Metadata: {}
--------------------------------------------------

Chunk 97:
Page Content: generation is to enable the model to grasp new concepts from a
minimal set of user-supplied images. Textual Inversion [243]
addresses this by finding a new pseudo-word S˚ (similar to
soft prompt discussed in Section III-A2) that represents new,
specific concepts in the textual embedding space of pre-trained
text-to-image diffusion models. The pseudo-word S˚ is opti-
mized via the original optimization goal in diffusion models
given a small image set (typically 3-5 images) depicting the
concept, and the pre-trained model is left untouched. During
inference, S˚can be treated like any other word and composed
with other textual queries (e.g., ”a photo of S˚ on the beach”).
Custom Diffusion [244] tackles a more challenging setting:
compositional fine-tuning of multiple concepts. It fine-tunes
only the Wk, Wv mapping from text to latent features in
attention layers, which yields superior performance in multi-
concept learning scenarios. Additionally, during fine-tuning,
Metadata: {}
--------------------------------------------------

Chunk 98:
Page Content: concept learning scenarios. Additionally, during fine-tuning,
Custom Diffusion prevents model forgetting by introducing
a small set of real images with captions akin to the target,
alongside employing augmentation for faster convergence and
improved results. IP-Adapter [245] identifies limitations in
current approaches (e.g., ControlNet and T2I-Adapter) which
project condition signals into the cross-attention modules.
When handling image conditions aiming at controlling content,
these methods are unable to generate images faithful to the
prompted image. The issue stems from that merging image
features and text features within cross-attention layers loses
image-specific information, leading to only coarse-grained
controllable generation such as image style rather than image
content. To overcome this, IP-Adapter introduces a novel
decoupled cross-attention mechanism to distinguish between
text and image features. IP-Adapter adds an additional cross-
Metadata: {}
--------------------------------------------------

Chunk 99:
Page Content: text and image features. IP-Adapter adds an additional cross-
attention layer exclusively for image features in each cross-
attention layer, and only the parameters of the new cross-
attention layers are trained.
VI. S YSTEM DESIGN CHALLENGE FOR PEFT
A. System design for PEFT
In this section, we begin by providing a concise overview
of cloud-based PEFT systems and analyzing the design chal-
lenges. These include the efficient handling of numerous task-
specific queries via centralized PEFT query servicing, the
resolution of privacy and data transmission issues through
distributed PEFT training, and the complexities associated
with concurrent multi-PEFT training processes. Centralized
systems are required to process a substantial volume of queries
with minimal latency and maximal throughput. Distributed
training frameworks must address privacy concerns and the
computational inefficiencies that arise from data exchanges
between users and cloud services. Furthermore, multi-PEFT
Metadata: {}
--------------------------------------------------

Chunk 100:
Page Content: between users and cloud services. Furthermore, multi-PEFT
training necessitates the optimization of memory utilization,
the management of simultaneous model training, and the
formulation of system architectures capable of supporting
multi-tenant workloads effectively. These challenges under-
score the imperative for innovative approaches to improve
scalability, safeguard privacy, and optimize resource allocation
in PEFT system architectures. Following this, we present the
corresponding metrics employed for evaluating the system
performance. Furthermore, we delve into three prospective
utilization scenarios to illustrate the challenges in system
design.
1) Centralized PEFT Query Serving: Cloud providers have
recently introduced a range of LLM services aimed at pro-
viding user applications through application programming
interfaces (APIs) [246], [247]. These APIs facilitate the seam-
less integration of many machine-learning functionalities into
Metadata: {}
--------------------------------------------------

Chunk 101:
Page Content: less integration of many machine-learning functionalities into
applications. When receiving one query for one specific down-
stream task through API, the cloud-based server processes the
query with one featured LLM model. Under this scenario, the
importance of PEFT becomes apparent. Cloud providers store
only a single copy of the LLM and multiple PEFT modules
featuring different downstream tasks. This setup allows the
LLM to maintain various branches of PEFT modules, each
linked to specific API queries, i.e., PEFT queries.
Centralized PEFT query serving solutions address scenarios
where multiple PEFT queries arrive in quick succession. A
case study of one state-of-the-art system for this purpose
is discussed in Section VI-B. Figure 10 (b) illustrates the
computation pattern for multi-query PEFT inference, wherein
packed PEFT queries are scheduled and executed according
to their deadlines and current system conditions.
2) Distributed PEFT Training: In most cases, personal-
Metadata: {}
--------------------------------------------------

Chunk 102:
Page Content: 2) Distributed PEFT Training: In most cases, personal-
ized tasks are not fully supported with pre-trained models,
17
LLMs
Edge Device Personal data
Cloud
Trainable Modules
🔥
Frozen Large Models
Scheduler
Request Pool
Query
Response
ExecutionEngine
Serving System
I like
I enjoy
LLM
programming
(a) (b)
Fig. 10: (a) Distributed-based system computation pattern; (b)
centralized PEFT Query inference.
consequently, extra fine-tuning is required to be executed
with the methodologies mentioned in the previous sections.
However, significant concerns arise when considering the
transfer of datasets to cloud providers, given the issues related
to data privacy, copyright, proprietary information, and the
complexities and inefficiencies involved in data transmission.
Section VI-C gives two approaches that address this concern.
3) Multi-PEFT Training: Different from multiple-PEFT
serving, tuning with multiple customized PEFTs always in-
volves different backbone LLMs. Therefore, simultaneously
Metadata: {}
--------------------------------------------------

Chunk 103:
Page Content: volves different backbone LLMs. Therefore, simultaneously
tuning multiple PEFTs can pose considerable challenges.
Challenges like how to manage memory gradient and model
weights storage, and how to design an efficient kernel for
batching PEFT training remain unsolved. PEFTs will be cat-
egorized based on their PEFT algorithms and backbone LLM
models. The design challenge involves how to consolidate
multiple PEFTs with the same LLM backbone and multiple
different LLM backbones simultaneously. We present case
studies related to this topic in Section VI-D.
4) Evaluation Metrics: For the proposed evaluation met-
rics, without loss of generality, we adopt large language
models as the basis for our metric definitions.
To evaluate the system performance of PEFT serving sys-
tems, we propose a set of evaluation metrics:
‚ System throughput: Considering PEFT queries as inter
and intra tasks, we use tokens per second to measure the
system throughput.
Metadata: {}
--------------------------------------------------

Chunk 104:
Page Content: and intra tasks, we use tokens per second to measure the
system throughput.
‚ Memory footprint: Run-time memory consumption dur-
ing query serving, the memory utilization comes from
both model parameters and KV-cache as mentioned in
Section IV-A.
‚ Accuracy performance : Real-world queries normally
have different context lengths, and performance with
variation length serves as a performance benchmark.
‚ Quality of services : Queries are associated with latency
requirements and deadline missing rates are considered
as another benchmark.
To assess the efficacy of PEFT training systems, we also
establish a set of evaluative metrics:
‚ Accuracy performance : Performance of the fine-tuned
model over the downstream tasks.
‚ Compute cost : The compute cost during forward and
backward propagation operations on cloud servers and
edge devices.
‚ Communication cost : Refers to the volume of data
involved during the transfer of intermediate data between
the edge device and the cloud.
Metadata: {}
--------------------------------------------------

Chunk 105:
Page Content: involved during the transfer of intermediate data between
the edge device and the cloud.
B. Centralized PEFT Serving Frameworks
The PEFT algorithm is notable for its ability to distin-
guish between modifiable and immutable weights within a
model. This characteristic inspires developers to amalgamate
diverse LLMs with distinct PEFT techniques into collective
units. PetS, as introduced in [248], advocates for a com-
prehensive approach to managing multiple PEFT tasks by
suggesting a unified serving framework. The framework’s
core advancement lies in the translation of varying PEFT
tasks into integrated computation kernels to enhance efficiency.
Moreover, PetS pioneers an orchestrated batching approach
and a scheduling methodology, aiming to augment system
throughput and leverage task parallelism respectively.
As depicted in Figure 11, the PetS framework begins
with users registering PEFT tasks through a standardized
Application Programming Interface (API). Upon registration,
Metadata: {}
--------------------------------------------------

Chunk 106:
Page Content: Application Programming Interface (API). Upon registration,
developers are expected to provide the Pre-Trained Model Tag
(e.g., LLaMA), PEFT parameters in a compressed format,
and the specific PEFT algorithms (e.g., LoRA, Adapter, Bitfit,
etc.). These tasks are then endowed with unique identifiers,
and the inference engine takes charge of query processing.
PetS bifurcates the primary computational workload (e.g.,
linear layer computations) into three distinct computational
operations: (1) Dense Matrix-Vector Multiplication (MVM)
leveraging universally accessible, pre-trained weights. (2) Bias
vector addition (Vadd), using either common or task-exclusive
biases. (3) A combination of Sparse/dense MVM operations
employing task-specific PET parameters. A unified pre-trained
weight matrix W is employed across PetS, facilitating the
batching of initial operations, Xt ˆW. However, subsequent
task-specific computations involving PET parameters, despite
Metadata: {}
--------------------------------------------------

Chunk 107:
Page Content: task-specific computations involving PET parameters, despite
being relatively minimal in complexity, are processed individ-
ually.
Considering the Adapter and Bitfit tasks as an illustration,
both aim at the MLP component of LLMs. The Adapter
task integrates additional weight segments, whereas Bitfit
adjusts bias elements. The Adapter operation is modeled as
Y “ Xin1 ˆpW `Wadq` b0, where Xin1 represents the
input for the Adapter task, W and Wad are the original
and adapter-specific PEFT weights respectively, and b0 is the
initial bias. The Bitfit operation, on the other hand, is defined
as Y “ Xin2 ˆW `b1, with b1 symbolizing the Bitfit-
adjustable bias. These operations are further synthesized as
tY1, Y2u“t Xin1, Xin2uˆ W `tXin1 ˆWad, 0u`t b0, b1u,
delineating that the tXin1, Xin2uˆ W part is amenable to
batching through MVM, while the tb0, b1u segment pertains
to the Vadd operation.
For tasks like Diff-Pruning III-B, is a little bit different
Metadata: {}
--------------------------------------------------

Chunk 108:
Page Content: to the Vadd operation.
For tasks like Diff-Pruning III-B, is a little bit different
than Bitfit and Adapter. For Diff-Pruning, the computation
concerning the shared weight and ‘difference’ are conducted
separately. Then the results are added up, namely
Xt ˆpW `δtq“ Xt ˆW `Xt ˆδt
, here, the W denotes the backbone model weights while δt
denotes the pruned weights which can be represented as Sparse
MVM.
18
PetSOverviewPET ServingPET Inference Pipeline
Pre-train Model IDShadow ParametersPET Type
Pre-train Model IDShadow ParametersPET Type
Pre-trained Model TagPET ParametersPET Type
PET ParametersShared ModelParameters
Register Tasksu Task RegisterPET Manager
Task Manager
Parameter Repository
v w
<Task_id> <Input Data>…
Query 0:
Query 1:
Input Queriesx Performance ModelBatch Scheduler
Scheduling Policy
EnginePET TaskSchedulerPET OperatorLibrary
yInput AnalyzingInput Reformatting
User Inputs<Task_id> <Input Data>
Fig. 11: PetS system overview: (1) Tasks register; (2) Task
Metadata: {}
--------------------------------------------------

Chunk 109:
Page Content: User Inputs<Task_id> <Input Data>
Fig. 11: PetS system overview: (1) Tasks register; (2) Task
manager (3) Task schedule; (4) Task serving. (Image is taken
from PetS [248])
Task 0 Task 4
Step 1: Intra-Task Batching
Task 1 Task 2 Task 3Mini
Batch
𝛽 − Model
𝛼 −Model
PET-OPs Profiling
Batch 1Batch 0
B=2, S=34
Step 2: Inter-Task Batching
Batch 2
Macro
Batch
Shared-OPs
Profiling
B=4, S=34
Task 0 Task 1 Task 3Task 2 Task 4 Fig. 12: Coordinated Batching (CB) Strategy
The other challenge PetS proposed is how to schedule dif-
ferent PEFT requests to achieve high performance. PetS sched-
uler achieves high parallelism through a two-level scheduling
policy: Coordinated Batching (CB) and Macro-batch Stream-
ing (MS) as Figure 12 depicts. Through CB, the input queries
will first be clustered based on their input length and then
grouped based on their shared operator. This is to make sure
the same sequence length of queries will be executed without
Metadata: {}
--------------------------------------------------

Chunk 110:
Page Content: the same sequence length of queries will be executed without
wasting padding. MS strategy will take the grouped queries
after coordinated batching and the theoretical latency for
different operators as well as the system modeling parameters
to generate the best execution order.
The other example design is DLoRA [249], which intro-
duces a system that improves the efficiency of serving low-
rank adaptation (LoRA) models for large language models
(LLMs) by dynamically managing the merging and unmerging
of LoRA adapters and the migration of requests across worker
replicas. This dynamic orchestration addresses the challenges
of high memory footprints, low GPU utilization, and load
imbalance caused by variable input and output lengths in
traditional LLM serving systems. dLoRA’s novel approaches,
including a credit-based batching algorithm and a request-
adapter co-migration algorithm, significantly enhance through-
put.
C. Distributed PEFT Training Frameworks
Metadata: {}
--------------------------------------------------

Chunk 111:
Page Content: put.
C. Distributed PEFT Training Frameworks
We already know that fine-tuning LLM for downstream
tasks is challenging for two reasons: dual privacy concerns
between cloud server and data owner, and issues with com-
putational resources and efficiency. Firstly, the privacy of
both parties is at risk: the weights of large models are often
proprietary and not made public. Sharing data with model
owners for fine-tuning can lead to data privacy concerns while
providing model weights to data proprietors could compromise
the ownership of proprietary models. Secondly, even if down-
stream users have access to pre-trained weights, the stringent
hardware requirements make transfer learning impractical for
most end users.
To resolve these two issues, DLoRA [250] presents a
distributed PEFT framework. During the PEFT process, the
backbone LLM is executed in the cloud servers while the
PEFT modules are trained entirely within the user devices.
DLoRA scheme is depicted in Figure 10(a).
Metadata: {}
--------------------------------------------------

Chunk 112:
Page Content: DLoRA scheme is depicted in Figure 10(a).
Similarly, Offsite-Tuning [251] presents a privacy-
preserving and efficient transfer learning framework that
enables foundational models to adapt to downstream tasks
without the need to access the complete model weights. The
key insight of Offsite-Tuning is the cloud provider sends an
adapter and an emulator to the data proprietor. Then, with the
assistance of the emulator, the data proprietor fine-tunes the
adapter. The fine-tuned adapter is then sent back to the cloud
side, which integrates it into the complete model, creating a
fine-tuned foundational model for downstream users. Offsite-
Tuning safeguards the privacy of data proprietors since they
do not need to share their training data directly. It also
protects the foundational model owners, as the complete model
weights are not shared, and the emulator provided is lossy,
with significantly degraded performance. Compared to existing
Metadata: {}
--------------------------------------------------

Chunk 113:
Page Content: with significantly degraded performance. Compared to existing
fine-tuning methods that require access to the full model
weights, Offsite-Tuning is more resource-efficient because it
allows for fine-tuning through a compressed emulator without
needing the complete model.
D. Parallel PEFT Training Frameworks
Unlike the PEFT query serving system, which aims to
accommodate flexible multi-PEFT algorithms, Punica [252]
focuses solely on facilitating multiple-LoRA blocks for various
tasks. Designing multiple PEFT training systems presents key
challenges in two main aspects:
‚ Efficient concurrent execution of multiple PEFT models
with the same LLM backbone.
‚ Designing an efficient system for multi-tenant serving
with different LLM backbones.
a) Efficient kernel design: Punica addresses the first chal-
lenge by using existing matrix multiplication for the backbone
computation and introducing a new CUDA kernel, Segmented
Gather Matrix-Vector Multiplication (SGMV), for adding the
Metadata: {}
--------------------------------------------------

Chunk 114:
Page Content: Gather Matrix-Vector Multiplication (SGMV), for adding the
PEFT add-ons to the backbone computation in a batched
manner. This kernel parallelizes the feature-weight multipli-
cation for different requests in the batch and groups requests
corresponding to the same PEFT model to increase operational
intensity and use GPU Tensor Cores for acceleration.
19
The second challenge is beyond the computational cost,
designing an efficient system architecture that can effectively
serve multi-tenant PEFT model workloads on the smallest set
of GPUs possible while occupying the least amount of GPU
resources is another significant challenge. Punica addresses
this by scheduling user requests to active GPUs that already
serve or train PEFT models, thereby improving GPU utiliza-
tion. For older requests, Punica periodically migrates them to
consolidate workloads, thus freeing up GPU resources for new
requests.
b) Multi-Tenant PEFT design: Designing an efficient
Metadata: {}
--------------------------------------------------

Chunk 115:
Page Content: requests.
b) Multi-Tenant PEFT design: Designing an efficient
system for the multi-tenant PEFT model serving in the Punica
framework focuses on addressing several key challenges to
maximize hardware utilization and minimize resource con-
sumption. The system aims to consolidate multi-tenant LoRA
serving workloads onto the smallest set of GPUs possible. This
consolidation is achieved through strategic scheduling of user
requests to active GPUs that are already serving or training
LoRA models, thereby improving GPU utilization. For older
requests, Punica periodically migrates them to consolidate
workloads further, thus freeing up GPU resources for new
requests. It incorporates on-demand loading of LoRA model
weights, which introduces only millisecond-level latency. This
feature provides Punica with the flexibility to dynamically
consolidate user requests to a small set of GPUs, without being
constrained by the specific LoRA models already running on
Metadata: {}
--------------------------------------------------

Chunk 116:
Page Content: constrained by the specific LoRA models already running on
those GPUs. Besides that, Punica identifies that the decode
stage is a predominant factor in the cost of model serving,
Punica’s design primarily focuses on optimizing decode stage
performance. Other aspects of model serving leverage straight-
forward techniques, such as on-demand loading of LoRA
model weights, to efficiently manage resource utilization.
VII. C ONCLUSION AND FUTURE DIRECTIONS
In the current era dominated by large models and large
datasets, PEFT stands out as a highly attractive method for
efficiently adapting models to downstream tasks. This tech-
nique gains its appeal by addressing the significant challenges
posed by traditional full-model fine-tuning, which often places
substantial computational and data demands. This survey of-
fers a comprehensive examination of the most recent advance-
ments in PEFT, including algorithmic design, computational
efficiency, application scenarios, and system implementation
Metadata: {}
--------------------------------------------------

Chunk 117:
Page Content: efficiency, application scenarios, and system implementation
for PEFT. It offers a comprehensive taxonomy and explanation
that serves as an excellent guidance and knowledge base,
which enables readers of various levels and disciplines to
swiftly grasp the core concepts of PEFT.
For further research on PEFT, we propose a series of pos-
sible directions from both algorithm and system perspectives,
hoping to inspire more researchers to engage in further studies
in these areas.
A. Simplify hyperparameter tuning
The effectiveness of PEFT is often sensitive to its hyperpa-
rameters, such as the bottleneck dimension of the adapter, the
rank of LoRA, and the arrangement of various additive PEFT
layers. Manually tuning these hyperparameters will cost lots
of effort. Therefore, future efforts could focus on developing
methods that are less dependent on manual tuning of these
parameters, or automatically find the optimal configuration
Metadata: {}
--------------------------------------------------

Chunk 118:
Page Content: parameters, or automatically find the optimal configuration
settings. Several studies [82], [83], [84], [98], [99], [100] have
started to address this issue, but there’s a need for more simple
and efficient solutions optimizing these hyperparameters.
B. Establish a unified benchmark
Despite the existence of libraries like HuggingFace’s
PEFT [253] and AdapterHub [254], a comprehensive bench-
mark for PEFT is still lacking. This gap hinders the ability
to fairly compare the performance and efficiency of different
PEFT approaches. A well-accepted, up-to-date benchmark
akin to MMDetection [255] for object detection would enable
researchers to validate their methods against a standard set
of tasks and metrics, fostering innovation and collaboration
within the community.
C. Enhance training efficiency
The presumed parameter efficiency of PEFT is not always
consistent with computational and memory savings during
training. Given that trainable parameters are intertwined within
Metadata: {}
--------------------------------------------------

Chunk 119:
Page Content: training. Given that trainable parameters are intertwined within
the pre-trained model’s architecture, computing and storing
activations and gradients for the full model often become
necessary during fine-tuning. This oversight calls for a rethink-
ing of what constitutes efficiency. As outlined in Section IV,
potential solutions lie in the integration of model compres-
sion techniques such as pruning and quantization, alongside
innovations specifically designed to optimize memory during
PEFT tuning [256]. Further research into enhancing the com-
putational efficiency of PEFT methodologies is imperative.
D. Explore scaling laws
The design and effectiveness of PEFT methods originally
developed for smaller Transformer models do not necessarily
scale with larger models. As the size of foundation models
increases, identifying and adapting PEFT strategies that remain
effective is crucial. This investigation will aid in customizing
PEFT methodologies to suit the evolving landscape of large
Metadata: {}
--------------------------------------------------

Chunk 120:
Page Content: PEFT methodologies to suit the evolving landscape of large
model architectures.
E. Serve more models and tasks
The rise of large foundation models across various domains
presents new opportunities for PEFT. Designing PEFT meth-
ods tailored to the unique characteristics of models, such as
Sora [257], Mamba [258], and LVM [259], can unlock new
application scenarios and opportunities.
F . Enhancing data privacy
Trusting centralized systems to serve or fine-tune personal-
ized PEFT modules is yet another issue for system developers.
Multiple types of inversion attacks [260], [261] have been pro-
posed to reconstruct user’s data by hijacking the intermediate
results. One perspective of future trust-worthy LLM system
design involves developing an encryption protocol for both
personal data and intermediate training and inference results.
20
G. PEFT with model compression
Model compression is one of the most effective ways to
make LLM executable on resource-limited devices. Yet, the
Metadata: {}
--------------------------------------------------

Chunk 121:
Page Content: make LLM executable on resource-limited devices. Yet, the
impact of model compression techniques on the performance
of PEFT algorithms running on hardware remains another
systemic challenge. Common compression techniques such
as quantization and pruning necessitate dedicated hardware
platforms to expedite the process, and building such hardware
platforms for compressed models is yet another direction for
future research.
REFERENCES
[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-
els are few-shot learners,” Advances in neural information processing
systems, vol. 33, pp. 1877–1901, 2020.
[2] Y . Zhuang, Y . Yu, K. Wang, H. Sun, and C. Zhang, “Toolqa: A
dataset for llm question answering with external tools,” arXiv preprint
arXiv:2306.13304, 2023.
[3] W. Zhu, H. Liu, Q. Dong, J. Xu, L. Kong, J. Chen, L. Li, and S. Huang,
“Multilingual machine translation with large language models: Empir-
Metadata: {}
--------------------------------------------------

Chunk 122:
Page Content: “Multilingual machine translation with large language models: Empir-
ical results and analysis,” arXiv preprint arXiv:2304.04675 , 2023.
[4] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. Shaikh,
N. Akhtar, J. Wu, and S. Mirjalili, “A survey on large language models:
Applications, challenges, limitations, and practical usage,” TechRxiv,
2023.
[5] B. Xu, X. Liu, H. Shen, Z. Han, Y . Li, M. Yue, Z. Peng, Y . Liu, Z. Yao,
and D. Xu, “Gentopia: A collaborative platform for tool-augmented
llms,” arXiv preprint arXiv:2308.04030 , 2023.
[6] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem,
“Camel: Communicative agents for ”mind” exploration of large lan-
guage model society,” in Thirty-seventh Conference on Neural Infor-
mation Processing Systems , 2023.
[7] Q. Wu, G. Bansal, J. Zhang, Y . Wu, S. Zhang, E. Zhu, B. Li,
L. Jiang, X. Zhang, and C. Wang, “Autogen: Enabling next-gen llm
applications via multi-agent conversation framework,” arXiv preprint
Metadata: {}
--------------------------------------------------

Chunk 123:
Page Content: applications via multi-agent conversation framework,” arXiv preprint
arXiv:2308.08155, 2023.
[8] H. Zhang, X. Liu, and J. Zhang, “Summit: Iterative text summarization
via chatgpt,” arXiv preprint arXiv:2305.14835 , 2023.
[9] B. Zhang and R. Sennrich, “Root mean square layer normalization,”
Advances in Neural Information Processing Systems , vol. 32, 2019.
[10] J. Su, Y . Lu, S. Pan, A. Murtadha, B. Wen, and Y . Liu, “Roformer:
Enhanced transformer with rotary position embedding,” arXiv preprint
arXiv:2104.09864, 2021.
[11] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,
“Glue: A multi-task benchmark and analysis platform for natural
language understanding,” arXiv preprint arXiv:1804.07461 , 2018.
[12] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of armor
conduct electricity? a new dataset for open book question answering,”
in EMNLP, 2018.
[13] Y . Bisk, R. Zellers, R. L. Bras, J. Gao, and Y . Choi, “Piqa: Reasoning
Metadata: {}
--------------------------------------------------

Chunk 124:
Page Content: in EMNLP, 2018.
[13] Y . Bisk, R. Zellers, R. L. Bras, J. Gao, and Y . Choi, “Piqa: Reasoning
about physical commonsense in natural language,” in Thirty-Fourth
AAAI Conference on Artificial Intelligence , 2020.
[14] M. Sap, H. Rashkin, D. Chen, R. LeBras, and Y . Choi, “Socialiqa:
Commonsense reasoning about social interactions,” arXiv preprint
arXiv:1904.09728, 2019.
[15] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi, “Hellaswag:
Can a machine really finish your sentence?” in Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics ,
2019.
[16] C. e. a. Clark, “Boolq: Exploring the surprising difficulty of natural
yes/no questions,” in NAACL, 2019.
[17] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y . Choi, “Winogrande:
An adversarial winograd schema challenge at scale,” Communications
of the ACM , vol. 64, no. 9, pp. 99–106, 2021.
[18] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
Metadata: {}
--------------------------------------------------

Chunk 125:
Page Content: [18] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
and O. Tafjord, “Think you have solved question answering? try arc,
the ai2 reasoning challenge,” arXiv:1803.05457v1, 2018.
[19] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijaya-
narasimhan, F. Viola, T. Green, T. Back, P. Natsev et al., “The kinetics
human action video dataset,” arXiv preprint arXiv:1705.06950 , 2017.
[20] R. Goyal, S. Ebrahimi Kahou, V . Michalski, J. Materzynska, S. West-
phal, H. Kim, V . Haenel, I. Fruend, P. Yianilos, M. Mueller-Freitag
et al. , “The” something something” video database for learning and
evaluating visual common sense,” in Proceedings of the IEEE interna-
tional conference on computer vision , 2017, pp. 5842–5850.
[21] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, “Hmdb: a
large video database for human motion recognition,” in 2011 Interna-
tional conference on computer vision . IEEE, 2011, pp. 2556–2563.
Metadata: {}
--------------------------------------------------

Chunk 126:
Page Content: tional conference on computer vision . IEEE, 2011, pp. 2556–2563.
[22] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll ´ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 .
Springer, 2014, pp. 740–755.
[23] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba,
“Scene parsing through ade20k dataset,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2017, pp. 633–
641.
[24] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser-
man, “The pascal visual object classes (voc) challenge,” International
journal of computer vision , vol. 88, pp. 303–338, 2010.
[25] N. Ding, Y . Qin, G. Yang, F. Wei, Z. Yang, Y . Su, S. Hu, Y . Chen, C.-
M. Chan, W. Chen et al., “Parameter-efficient fine-tuning of large-scale
Metadata: {}
--------------------------------------------------

Chunk 127:
Page Content: M. Chan, W. Chen et al., “Parameter-efficient fine-tuning of large-scale
pre-trained language models,” Nature Machine Intelligence , vol. 5,
no. 3, pp. 220–235, 2023.
[26] L. Xu, H. Xie, S.-Z. J. Qin, X. Tao, and F. L. Wang, “Parameter-
efficient fine-tuning methods for pretrained language models: A critical
review and assessment,” arXiv preprint arXiv:2312.12148 , 2023.
[27] G. Pu, A. Jain, J. Yin, and R. Kaplan, “Empirical analysis of the
strengths and weaknesses of peft techniques for llms,” arXiv preprint
arXiv:2304.14999, 2023.
[28] OpenAI, “Sharegpt,” https://sharegpt.com/, 2023.
[29] Microsoft, “Microsoft azure function trace,”
https://github.com/Azure/AzurePublicDataset, 2023.
[30] I. S. Moreno, P. Garraghan, P. Townend, and J. Xu, “Analysis, modeling
and simulation of workload patterns in a large-scale utility cloud,”IEEE
Transactions on Cloud Computing , vol. 2, no. 2, pp. 208–221, 2014.
[31] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,
Metadata: {}
--------------------------------------------------

Chunk 128:
Page Content: [31] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,
A. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-efficient transfer
learning for nlp,” in International Conference on Machine Learning .
PMLR, 2019, pp. 2790–2799.
[32] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, “Towards
a unified view of parameter-efficient transfer learning,” arXiv preprint
arXiv:2110.04366, 2021.
[33] Y . Zhu, J. Feng, C. Zhao, M. Wang, and L. Li, “Counter-
interference adapter for multilingual machine translation,” arXiv
preprint arXiv:2104.08154, 2021.
[34] T. Lei, J. Bai, S. Brahma, J. Ainslie, K. Lee, Y . Zhou, N. Du, V . Y .
Zhao, Y . Wu, B. Li et al. , “Conditional adapters: Parameter-efficient
transfer learning with fast inference,” arXiv preprint arXiv:2304.04947,
2023.
[35] J. Pfeiffer, A. Kamath, A. R ¨uckl´e, K. Cho, and I. Gurevych, “Adapter-
fusion: Non-destructive task composition for transfer learning,” arXiv
preprint arXiv:2005.00247, 2020.
Metadata: {}
--------------------------------------------------

Chunk 129:
Page Content: preprint arXiv:2005.00247, 2020.
[36] Y . Wang, S. Mukherjee, X. Liu, J. Gao, A. H. Awadallah, and J. Gao,
“Adamix: Mixture-of-adapter for parameter-efficient tuning of large
language models,” arXiv preprint arXiv:2205.12410, vol. 1, no. 2, p. 4,
2022.
[37] H. Zhao, J. Fu, and Z. He, “Prototype-based hyperadapter for sample-
efficient multi-task tuning,” arXiv preprint arXiv:2310.11670 , 2023.
[38] A. Chronopoulou, M. E. Peters, A. Fraser, and J. Dodge, “Adaptersoup:
Weight averaging to improve generalization of pretrained language
models,” arXiv preprint arXiv:2302.07027 , 2023.
[39] S. He, R.-Z. Fan, L. Ding, L. Shen, T. Zhou, and D. Tao, “Mera:
Merging pretrained adapters for few-shot learning,” arXiv preprint
arXiv:2308.15982, 2023.
[40] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson, “Parameter-
efficient multi-task fine-tuning for transformers via shared hypernet-
works,” arXiv preprint arXiv:2106.04489 , 2021.
Metadata: {}
--------------------------------------------------

Chunk 130:
Page Content: works,” arXiv preprint arXiv:2106.04489 , 2021.
[41] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts
for generation,” arXiv preprint arXiv:2101.00190 , 2021.
[42] J. Li, W. Aitken, R. Bhambhoria, and X. Zhu, “Prefix propaga-
tion: Parameter-efficient tuning for long sequences,” arXiv preprint
arXiv:2305.12086, 2023.
[43] X. Liu, K. Ji, Y . Fu, W. L. Tam, Z. Du, Z. Yang, and J. Tang, “P-tuning
v2: Prompt tuning can be comparable to fine-tuning universally across
scales and tasks,” arXiv preprint arXiv:2110.07602 , 2021.
21
[44] Z.-R. Zhang, C. Tan, H. Xu, C. Wang, J. Huang, and S. Huang,
“Towards adaptive prefix tuning for parameter-efficient language model
fine-tuning,” arXiv preprint arXiv:2305.15212 , 2023.
[45] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang, “Gpt
understands, too,” arXiv preprint arXiv:2103.10385 , 2021.
[46] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for
Metadata: {}
--------------------------------------------------

Chunk 131:
Page Content: [46] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for
parameter-efficient prompt tuning,” arXiv preprint arXiv:2104.08691 ,
2021.
[47] F. Ma, C. Zhang, L. Ren, J. Wang, Q. Wang, W. Wu, X. Quan, and
D. Song, “Xprompt: Exploring the extreme of prompt tuning,” arXiv
preprint arXiv:2210.04457, 2022.
[48] Z. Wu, S. Wang, J. Gu, R. Hou, Y . Dong, V . Vydiswaran, and
H. Ma, “Idpg: An instance-dependent prompt generation method,”
arXiv preprint arXiv:2204.04497 , 2022.
[49] X. Liu, T. Sun, X. Huang, and X. Qiu, “Late prompt tuning: A
late prompt could be better than many prompts,” arXiv preprint
arXiv:2210.11292, 2022.
[50] W. Zhu and M. Tan, “Spt: Learning to selectively insert prompts
for better prompt tuning,” in Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing, 2023, pp. 11 862–
11 878.
[51] Q. Wang, Y . Mao, J. Wang, H. Yu, S. Nie, S. Wang, F. Feng, L. Huang,
X. Quan, Z. Xu et al., “Aprompt: Attention prompt tuning for efficient
Metadata: {}
--------------------------------------------------

Chunk 132:
Page Content: X. Quan, Z. Xu et al., “Aprompt: Attention prompt tuning for efficient
adaptation of pre-trained language models,” in Proceedings of the 2023
Conference on Empirical Methods in Natural Language Processing ,
2023, pp. 9147–9160.
[52] T. Vu, B. Lester, N. Constant, R. Al-Rfou, and D. Cer, “Spot: Better
frozen model adaptation through soft prompt transfer,” arXiv preprint
arXiv:2110.07904, 2021.
[53] Y . Su, X. Wang, Y . Qin, C.-M. Chan, Y . Lin, H. Wang, K. Wen, Z. Liu,
P. Li, J. Li et al. , “On transferability of prompt tuning for natural
language processing,” arXiv preprint arXiv:2111.06719 , 2021.
[54] J. Wu, T. Yu, R. Wang, Z. Song, R. Zhang, H. Zhao, C. Lu, S. Li,
and R. Henao, “Infoprompt: Information-theoretic soft prompt tuning
for natural language understanding,” arXiv preprint arXiv:2306.04933,
2023.
[55] L. Chen, H. Huang, and M. Cheng, “Ptp: Boosting stability and
performance of prompt tuning with perturbation-based regularizer,”
arXiv preprint arXiv:2305.02423 , 2023.
Metadata: {}
--------------------------------------------------

Chunk 133:
Page Content: arXiv preprint arXiv:2305.02423 , 2023.
[56] Y . Qin, X. Wang, Y . Su, Y . Lin, N. Ding, J. Yi, W. Chen, Z. Liu, J. Li,
L. Hou et al., “Exploring universal intrinsic task subspace via prompt
tuning,” arXiv preprint arXiv:2110.07867 , 2021.
[57] J.-Y . Choi, J. Kim, J.-H. Park, W.-L. Mok, and S. Lee, “Smop: Towards
efficient and effective prompt tuning with sparse mixture-of-prompts,”
in Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing, 2023, pp. 14 306–14 316.
[58] Z. Shi and A. Lipani, “Dept: Decomposed prompt tuning for parameter-
efficient fine-tuning,” arXiv preprint arXiv:2309.05173 , 2023.
[59] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A.
Raffel, “Few-shot parameter-efficient fine-tuning is better and cheaper
than in-context learning,” Advances in Neural Information Processing
Systems, vol. 35, pp. 1950–1965, 2022.
[60] T. Zadouri, A. ¨Ust¨un, A. Ahmadian, B. Ermis ¸, A. Locatelli, and
Metadata: {}
--------------------------------------------------

Chunk 134:
Page Content: [60] T. Zadouri, A. ¨Ust¨un, A. Ahmadian, B. Ermis ¸, A. Locatelli, and
S. Hooker, “Pushing mixture of experts to the limit: Extremely
parameter efficient moe for instruction tuning,” arXiv preprint
arXiv:2309.05444, 2023.
[61] D. Lian, D. Zhou, J. Feng, and X. Wang, “Scaling & shifting your
features: A new baseline for efficient model tuning,” Advances in
Neural Information Processing Systems , vol. 35, pp. 109–123, 2022.
[62] X. Lu, F. Brahman, P. West, J. Jang, K. Chandu, A. Ravichander,
L. Qin, P. Ammanabrolu, L. Jiang, S. Ramnath et al., “Inference-time
policy adapters (ipa): Tailoring extreme-scale lms without fine-tuning,”
arXiv preprint arXiv:2305.15065 , 2023.
[63] D. Guo, A. M. Rush, and Y . Kim, “Parameter-efficient transfer learning
with diff pruning,” arXiv preprint arXiv:2012.07463 , 2020.
[64] N. Lawton, A. Kumar, G. Thattai, A. Galstyan, and G. V . Steeg,
“Neural architecture search for parameter-efficient fine-tuning of large
Metadata: {}
--------------------------------------------------

Chunk 135:
Page Content: “Neural architecture search for parameter-efficient fine-tuning of large
pre-trained language models,” arXiv preprint arXiv:2305.16597, 2023.
[65] B. Liao, Y . Meng, and C. Monz, “Parameter-efficient fine-tuning
without introducing new latency,” arXiv preprint arXiv:2305.16742 ,
2023.
[66] Y .-L. Sung, V . Nair, and C. A. Raffel, “Training neural networks
with fixed sparse masks,” Advances in Neural Information Processing
Systems, vol. 34, pp. 24 193–24 205, 2021.
[67] S. S. S. Das, R. H. Zhang, P. Shi, W. Yin, and R. Zhang, “Unified
low-resource sequence labeling by sample-aware dynamic sparse fine-
tuning,” arXiv preprint arXiv:2311.03748 , 2023.
[68] A. Ansell, E. M. Ponti, A. Korhonen, and I. Vuli ´c, “Compos-
able sparse fine-tuning for cross-lingual transfer,” arXiv preprint
arXiv:2110.07560, 2021.
[69] Z. Fu, H. Yang, A. M.-C. So, W. Lam, L. Bing, and N. Collier, “On
the effectiveness of parameter-efficient fine-tuning,” in Proceedings of
Metadata: {}
--------------------------------------------------

Chunk 136:
Page Content: the effectiveness of parameter-efficient fine-tuning,” in Proceedings of
the AAAI Conference on Artificial Intelligence , vol. 37, no. 11, 2023,
pp. 12 799–12 807.
[70] R. Xu, F. Luo, Z. Zhang, C. Tan, B. Chang, S. Huang, and F. Huang,
“Raise a child in large language model: Towards effective and gener-
alizable fine-tuning,” arXiv preprint arXiv:2109.05687 , 2021.
[71] D. Vucetic, M. Tayaranian, M. Ziaeefard, J. J. Clark, B. H. Meyer,
and W. J. Gross, “Efficient fine-tuning of bert models on the edge,” in
2022 IEEE International Symposium on Circuits and Systems (ISCAS) .
IEEE, 2022, pp. 1838–1842.
[72] E. B. Zaken, S. Ravfogel, and Y . Goldberg, “Bitfit: Simple parameter-
efficient fine-tuning for transformer-based masked language-models,”
arXiv preprint arXiv:2106.10199 , 2021.
[73] M. Gheini, X. Ren, and J. May, “Cross-attention is all you need: Adapt-
ing pretrained transformers for machine translation,” arXiv preprint
arXiv:2104.08771, 2021.
Metadata: {}
--------------------------------------------------

Chunk 137:
Page Content: ing pretrained transformers for machine translation,” arXiv preprint
arXiv:2104.08771, 2021.
[74] H. He, J. Cai, J. Zhang, D. Tao, and B. Zhuang, “Sensitivity-aware
visual parameter-efficient fine-tuning,” inProceedings of the IEEE/CVF
International Conference on Computer Vision , 2023, pp. 11 825–
11 835.
[75] A. Aghajanyan, L. Zettlemoyer, and S. Gupta, “Intrinsic dimension-
ality explains the effectiveness of language model fine-tuning,” arXiv
preprint arXiv:2012.13255, 2020.
[76] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,
and W. Chen, “Lora: Low-rank adaptation of large language models,”
arXiv preprint arXiv:2106.09685 , 2021.
[77] R. Karimi Mahabadi, J. Henderson, and S. Ruder, “Compacter: Ef-
ficient low-rank hypercomplex adapter layers,” Advances in Neural
Information Processing Systems , vol. 34, pp. 1022–1035, 2021.
[78] A. Edalati, M. Tahaei, I. Kobyzev, V . P. Nia, J. J. Clark, and M. Reza-
Metadata: {}
--------------------------------------------------

Chunk 138:
Page Content: [78] A. Edalati, M. Tahaei, I. Kobyzev, V . P. Nia, J. J. Clark, and M. Reza-
gholizadeh, “Krona: Parameter efficient tuning with kronecker adapter,”
arXiv preprint arXiv:2212.10650 , 2022.
[79] X. He, C. Li, P. Zhang, J. Yang, and X. E. Wang, “Parameter-efficient
model adaptation for vision transformers,” in Proceedings of the AAAI
Conference on Artificial Intelligence, vol. 37, no. 1, 2023, pp. 817–825.
[80] D. J. Kopiczko, T. Blankevoort, and Y . M. Asano, “Vera: Vector-based
random matrix adaptation,” arXiv preprint arXiv:2310.11454 , 2023.
[81] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-
T. Cheng, and M.-H. Chen, “Dora: Weight-decomposed low-rank
adaptation,” arXiv preprint arXiv:2402.09353 , 2024.
[82] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi, “Dylora:
Parameter efficient tuning of pre-trained models using dynamic search-
free low-rank adaptation,” arXiv preprint arXiv:2210.07558 , 2022.
Metadata: {}
--------------------------------------------------

Chunk 139:
Page Content: free low-rank adaptation,” arXiv preprint arXiv:2210.07558 , 2022.
[83] Q. Zhang, M. Chen, A. Bukharin, P. He, Y . Cheng, W. Chen, and
T. Zhao, “Adaptive budget allocation for parameter-efficient fine-
tuning,” arXiv preprint arXiv:2303.10512 , 2023.
[84] N. Ding, X. Lv, Q. Wang, Y . Chen, B. Zhou, Z. Liu, and M. Sun,
“Sparse low-rank adaptation of pre-trained language models,” arXiv
preprint arXiv:2311.11696, 2023.
[85] S. Haobo, H. Zhao, S. Majumder, and T. Lin, “Increasing model
capacity for free: A simple strategy for parameter efficient fine-tuning,”
in The Twelfth International Conference on Learning Representations ,
2023.
[86] R. Zhang, R. Qiang, S. A. Somayajula, and P. Xie, “Autolora: Auto-
matically tuning matrix ranks in low-rank adaptation based on meta
learning,” arXiv preprint arXiv:2403.09113 , 2024.
[87] A. X. Yang, M. Robeyns, X. Wang, and L. Aitchison, “Bayesian
low-rank adaptation for large language models,” arXiv preprint
arXiv:2308.13111, 2023.
Metadata: {}
--------------------------------------------------

Chunk 140:
Page Content: low-rank adaptation for large language models,” arXiv preprint
arXiv:2308.13111, 2023.
[88] Y . Lin, X. Ma, X. Chu, Y . Jin, Z. Yang, Y . Wang, and H. Mei, “Lora
dropout as a sparsity regularizer for overfitting control,” arXiv preprint
arXiv:2404.09610, 2024.
[89] X. Meng, D. Dai, W. Luo, Z. Yang, S. Wu, X. Wang, P. Wang, Q. Dong,
L. Chen, and Z. Sui, “Periodiclora: Breaking the low-rank bottleneck
in lora optimization,” arXiv preprint arXiv:2402.16141 , 2024.
[90] S. Hayou, N. Ghosh, and B. Yu, “Lora+: Efficient low rank adaptation
of large models,” arXiv preprint arXiv:2402.12354 , 2024.
[91] T. Wu, J. Wang, Z. Zhao, and N. Wong, “Mixture-of-subspaces in low-
rank adaptation,” arXiv preprint arXiv:2406.11909 , 2024.
[92] C. Huang, Q. Liu, B. Y . Lin, T. Pang, C. Du, and M. Lin, “Lorahub:
Efficient cross-task generalization via dynamic lora composition,”arXiv
preprint arXiv:2307.13269, 2023.
22
[93] Q. Liu, X. Wu, X. Zhao, Y . Zhu, D. Xu, F. Tian, and Y . Zheng,
Metadata: {}
--------------------------------------------------

Chunk 141:
Page Content: 22
[93] Q. Liu, X. Wu, X. Zhao, Y . Zhu, D. Xu, F. Tian, and Y . Zheng,
“Moelora: An moe-based parameter efficient fine-tuning method for
multi-task medical applications,” arXiv preprint arXiv:2310.18339 ,
2023.
[94] W. Feng, C. Hao, Y . Zhang, Y . Han, and H. Wang, “Mixture-of-loras:
An efficient multitask tuning for large language models,”arXiv preprint
arXiv:2403.03432, 2024.
[95] X. Wu, S. Huang, and F. Wei, “Mixture of lora experts,” arXiv preprint
arXiv:2404.13628, 2024.
[96] D. Li, Y . Ma, N. Wang, Z. Cheng, L. Duan, J. Zuo, C. Yang, and
M. Tang, “Mixlora: Enhancing large language models fine-tuning with
lora based mixture of experts,” arXiv preprint arXiv:2404.15159, 2024.
[97] Y . Mao, L. Mathias, R. Hou, A. Almahairi, H. Ma, J. Han, W.-t. Yih,
and M. Khabsa, “Unipelt: A unified framework for parameter-efficient
language model tuning,” arXiv preprint arXiv:2110.07577 , 2021.
[98] J. Chen, A. Zhang, X. Shi, M. Li, A. Smola, and D. Yang, “Parameter-
Metadata: {}
--------------------------------------------------

Chunk 142:
Page Content: [98] J. Chen, A. Zhang, X. Shi, M. Li, A. Smola, and D. Yang, “Parameter-
efficient fine-tuning design spaces,” arXiv preprint arXiv:2301.01821 ,
2023.
[99] Y . Zhang, K. Zhou, and Z. Liu, “Neural prompt search,” 2022.
[100] H. Zhou, X. Wan, I. Vuli ´c, and A. Korhonen, “Autopeft: Automatic
configuration search for parameter-efficient fine-tuning,” arXiv preprint
arXiv:2301.12132, 2023.
[101] Z. Hu, Y . Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing, and
S. Poria, “Llm-adapters: An adapter family for parameter-efficient fine-
tuning of large language models,” arXiv preprint arXiv:2304.01933 ,
2023.
[102] S. Hu, Z. Zhang, N. Ding, Y . Wang, Y . Wang, Z. Liu, and M. Sun,
“Sparse structure search for parameter-efficient tuning,” arXiv preprint
arXiv:2206.07382, 2022.
[103] A. Petrov, P. H. Torr, and A. Bibi, “When do prompting and prefix-
tuning work? a theory of capabilities and limitations,” arXiv preprint
arXiv:2310.19698, 2023.
Metadata: {}
--------------------------------------------------

Chunk 143:
Page Content: tuning work? a theory of capabilities and limitations,” arXiv preprint
arXiv:2310.19698, 2023.
[104] Y . Wang, J. Chauhan, W. Wang, and C.-J. Hsieh, “Universality and
limitations of prompt tuning,” arXiv preprint arXiv:2305.18787, 2023.
[105] Y . Choi and J.-H. Lee, “Codeprompt: Task-agnostic prefix tuning for
program and language generation,” in Findings of the Association for
Computational Linguistics: ACL 2023 , 2023, pp. 5282–5297.
[106] H. Wu and X. Shi, “Adversarial soft prompt tuning for cross-domain
sentiment analysis,” in Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers) ,
2022, pp. 2438–2447.
[107] J. Frankle and M. Carbin, “The lottery ticket hypothesis: Finding
sparse, trainable neural networks,” arXiv preprint arXiv:1803.03635 ,
2018.
[108] E. Malach, G. Yehudai, S. Shalev-Schwartz, and O. Shamir, “Proving
the lottery ticket hypothesis: Pruning is all you need,” in International
Metadata: {}
--------------------------------------------------

Chunk 144:
Page Content: the lottery ticket hypothesis: Pruning is all you need,” in International
Conference on Machine Learning . PMLR, 2020, pp. 6682–6691.
[109] V . Fomenko, H. Yu, J. Lee, S. Hsieh, and W. Chen, “A note on lora,”
arXiv preprint arXiv:2404.05086 , 2024.
[110] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholding
algorithm for linear inverse problems,” SIAM journal on imaging
sciences, vol. 2, no. 1, pp. 183–202, 2009.
[111] A. Chambolle, R. A. De V ore, N.-Y . Lee, and B. J. Lucier, “Nonlinear
wavelet image processing: variational problems, compression, and
noise removal through wavelet shrinkage,”IEEE Transactions on image
processing, vol. 7, no. 3, pp. 319–335, 1998.
[112] D. J. MacKay, “A practical bayesian framework for backpropagation
networks,” Neural computation, vol. 4, no. 3, pp. 448–472, 1992.
[113] J. Antor ´an, D. Janz, J. U. Allingham, E. Daxberger, R. R. Barbano,
E. Nalisnick, and J. M. Hern ´andez-Lobato, “Adapting the linearised
Metadata: {}
--------------------------------------------------

Chunk 145:
Page Content: E. Nalisnick, and J. M. Hern ´andez-Lobato, “Adapting the linearised
laplace model evidence for modern deep learning,” in International
Conference on Machine Learning . PMLR, 2022, pp. 796–821.
[114] J. Liu, A. Moreau, M. Preuss, J. Rapin, B. Roziere, F. Teytaud, and
O. Teytaud, “Versatile black-box optimization,” in Proceedings of the
2020 Genetic and Evolutionary Computation Conference , 2020, pp.
620–628.
[115] M. Chen, H. Peng, J. Fu, and H. Ling, “Autoformer: Searching
transformers for visual recognition,” in Proceedings of the IEEE/CVF
international conference on computer vision , 2021, pp. 12 270–12 280.
[116] P. I. Frazier, “A tutorial on bayesian optimization,” arXiv preprint
arXiv:1807.02811, 2018.
[117] A. R ¨uckl´e, G. Geigle, M. Glockner, T. Beck, J. Pfeiffer, N. Reimers,
and I. Gurevych, “Adapterdrop: On the efficiency of adapters in
transformers,” arXiv preprint arXiv:2010.11918 , 2020.
[118] S. He, L. Ding, D. Dong, J. Zhang, and D. Tao, “SparseAdapter: An
Metadata: {}
--------------------------------------------------

Chunk 146:
Page Content: [118] S. He, L. Ding, D. Dong, J. Zhang, and D. Tao, “SparseAdapter: An
easy approach for improving the parameter-efficiency of adapters,”
in Findings of the Association for Computational Linguistics:
EMNLP 2022 . Abu Dhabi, United Arab Emirates: Association
for Computational Linguistics, Dec. 2022, pp. 2184–2190. [Online].
Available: https://aclanthology.org/2022.findings-emnlp.160
[119] L. Hedegaard, A. Alok, J. Jose, and A. Iosifidis, “Structured pruning
adapters,” arXiv preprint arXiv:2211.10155 , 2022.
[120] M. Zhang, C. Shen, Z. Yang, L. Ou, X. Yu, B. Zhuang et al., “Prun-
ing meets low-rank parameter-efficient fine-tuning,” arXiv preprint
arXiv:2305.18403, 2023.
[121] G. Zeng, P. Zhang, and W. Lu, “One network, many masks: To-
wards more parameter-efficient transfer learning,” arXiv preprint
arXiv:2305.17682, 2023.
[122] S. Jie, H. Wang, and Z.-H. Deng, “Revisiting the parameter efficiency
of adapters from the perspective of precision redundancy,” in Proceed-
Metadata: {}
--------------------------------------------------

Chunk 147:
Page Content: of adapters from the perspective of precision redundancy,” in Proceed-
ings of the IEEE/CVF International Conference on Computer Vision ,
2023, pp. 17 217–17 226.
[123] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, and D. Lee,
“Memory-efficient fine-tuning of compressed large language models
via sub-4-bit integer quantization,” arXiv preprint arXiv:2305.14152 ,
2023.
[124] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Ef-
ficient finetuning of quantized llms,” arXiv preprint arXiv:2305.14314,
2023.
[125] Y . Li, Y . Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and
T. Zhao, “Loftq: Lora-fine-tuning-aware quantization for large language
models,” arXiv preprint arXiv:2310.08659 , 2023.
[126] H. Guo, P. Greengard, E. P. Xing, and Y . Kim, “Lq-lora: Low-rank
plus quantized matrix decomposition for efficient language model
finetuning,” arXiv preprint arXiv:2311.12023 , 2023.
[127] Y . Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen, X. Zhang,
Metadata: {}
--------------------------------------------------

Chunk 148:
Page Content: [127] Y . Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen, X. Zhang,
and Q. Tian, “Qa-lora: Quantization-aware low-rank adaptation of large
language models,” arXiv preprint arXiv:2309.14717 , 2023.
[128] Y . Chai, J. Gkountouras, G. G. Ko, D. Brooks, and G.-Y . Wei, “Int2. 1:
Towards fine-tunable quantized large language models with error cor-
rection through low-rank adaptation,”arXiv preprint arXiv:2306.08162,
2023.
[129] H. Rajabzadeh, M. Valipour, T. Zhu, M. Tahaei, H. J. Kwon, A. Ghodsi,
B. Chen, and M. Rezagholizadeh, “Qdylora: Quantized dynamic low-
rank adaptation for efficient large language model tuning,” arXiv
preprint arXiv:2402.10462, 2024.
[130] J. Liu, G. Xiao, K. Li, J. D. Lee, S. Han, T. Dao, and T. Cai,
“Bitdelta: Your fine-tune may only be worth one bit,” arXiv preprint
arXiv:2402.10193, 2024.
[131] J. O. Zhang, A. Sax, A. Zamir, L. Guibas, and J. Malik, “Side-
tuning: a baseline for network adaptation via additive side networks,”
Metadata: {}
--------------------------------------------------

Chunk 149:
Page Content: tuning: a baseline for network adaptation via additive side networks,”
in Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part III 16 . Springer, 2020,
pp. 698–714.
[132] Y .-L. Sung, J. Cho, and M. Bansal, “Lst: Ladder side-tuning for
parameter and memory efficient transfer learning,” Advances in Neural
Information Processing Systems , vol. 35, pp. 12 991–13 005, 2022.
[133] Z. Jiang, C. Mao, Z. Huang, A. Ma, Y . Lv, Y . Shen, D. Zhao, and
J. Zhou, “Res-tuning: A flexible and efficient tuning paradigm via
unbinding tuner from backbone,” arXiv preprint arXiv:2310.19859 ,
2023.
[134] B. Liao, S. Tan, and C. Monz, “Make your pre-trained model reversible:
From parameter to memory efficient fine-tuning,” arXiv preprint
arXiv:2306.00477, 2023.
[135] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li, “Lora-fa: Memory-
efficient low-rank adaptation for large language models fine-tuning,”
arXiv preprint arXiv:2308.03303 , 2023.
Metadata: {}
--------------------------------------------------

Chunk 150:
Page Content: arXiv preprint arXiv:2308.03303 , 2023.
[136] J. Phang, Y . Mao, P. He, and W. Chen, “Hypertuning: Toward adapting
large language models without back-propagation,” in International
Conference on Machine Learning . PMLR, 2023, pp. 27 854–27 875.
[137] F. Jin, J. Zhang, and C. Zong, “Parameter-efficient tuning for large
language model without calculating its gradients,” in Proceedings of
the 2023 Conference on Empirical Methods in Natural Language
Processing, 2023, pp. 321–330.
[138] S. Malladi, T. Gao, E. Nichani, A. Damian, J. D. Lee, D. Chen, and
S. Arora, “Fine-tuning language models with just forward passes,”
arXiv preprint arXiv:2305.17333 , 2023.
[139] J. Zhao, Z. Zhang, B. Chen, Z. Wang, A. Anandkumar, and Y . Tian,
“Galore: Memory-efficient llm training by gradient low-rank projec-
tion,” arXiv preprint arXiv:2403.03507 , 2024.
[140] Y . Sheng, S. Cao, D. Li, C. Hooper, N. Lee, S. Yang, C. Chou, B. Zhu,
L. Zheng, K. Keutzer et al., “S-lora: Serving thousands of concurrent
Metadata: {}
--------------------------------------------------

Chunk 151:
Page Content: L. Zheng, K. Keutzer et al., “S-lora: Serving thousands of concurrent
lora adapters,” arXiv preprint arXiv:2311.03285 , 2023.
23
[141] T. Zhou and D. Tao, “Godec: Randomized low-rank & sparse matrix
decomposition in noisy case,” in Proceedings of the 28th International
Conference on Machine Learning, ICML 2011 , 2011.
[142] J. Wright, A. Ganesh, S. Rao, Y . Peng, and Y . Ma, “Robust principal
component analysis: Exact recovery of corrupted low-rank matrices
via convex optimization,” Advances in neural information processing
systems, vol. 22, 2009.
[143] A. N. Gomez, M. Ren, R. Urtasun, and R. B. Grosse, “The reversible
residual network: Backpropagation without storing activations,” Ad-
vances in neural information processing systems , vol. 30, 2017.
[144] Y . Huang, Y . Li, Y . Xu, L. Zhang, R. Gan, J. Zhang, and L. Wang,
“Mvp-tuning: Multi-view knowledge retrieval with prompt tuning for
commonsense reasoning,” in Proceedings of the 61st Annual Meeting
Metadata: {}
--------------------------------------------------

Chunk 152:
Page Content: commonsense reasoning,” in Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long
Papers), 2023, pp. 13 417–13 432.
[145] Z. Zhao, L. Hu, H. Zhao, Y . Shao, and Y . Wang, “Knowledgeable
parameter efficient tuning network for commonsense question answer-
ing,” in Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , 2023, pp. 9051–
9063.
[146] H. Zhao, R. He, M. Xiao, and J. Xu, “Infusing hierarchical guidance
into prompt tuning: A parameter-efficient framework for multi-level
implicit discourse relation recognition,” in Proceedings of the 61st
Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), 2023, pp. 6477–6492.
[147] Y . Ouyang, Y . Cao, Y . Gao, Z. Wu, J. Zhang, and X. Dai, “On prefix-
tuning for lightweight out-of-distribution detection,” in Proceedings
of the 61st Annual Meeting of the Association for Computational
Metadata: {}
--------------------------------------------------

Chunk 153:
Page Content: of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , 2023, pp. 1533–1545.
[148] M. S. Ozdayi, C. Peris, J. Fitzgerald, C. Dupuy, J. Majmudar, H. Khan,
R. Parikh, and R. Gupta, “Controlling the extraction of memorized
data from large language models via prompt-tuning,” arXiv preprint
arXiv:2305.11759, 2023.
[149] G. Xiao, J. Lin, and S. Han, “Offsite-tuning: Transfer learning without
full model,” arXiv preprint arXiv:2302.04870 , 2023.
[150] T. Che, J. Liu, Y . Zhou, J. Ren, J. Zhou, V . S. Sheng, H. Dai, and
D. Dou, “Federated learning of large language models with parameter-
efficient prompt tuning and adaptive optimization,” arXiv preprint
arXiv:2310.15080, 2023.
[151] Y . Li, M. Du, X. Wang, and Y . Wang, “Prompt tuning pushes farther,
contrastive learning pulls closer: A two-stage approach to mitigate
social biases,” arXiv preprint arXiv:2307.01595 , 2023.
[152] J. Cho, J. Lei, H. Tan, and M. Bansal, “Unifying vision-and-language
Metadata: {}
--------------------------------------------------

Chunk 154:
Page Content: [152] J. Cho, J. Lei, H. Tan, and M. Bansal, “Unifying vision-and-language
tasks via text generation,” in International Conference on Machine
Learning. PMLR, 2021, pp. 1931–1942.
[153] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-4: En-
hancing vision-language understanding with advanced large language
models,” arXiv preprint arXiv:2304.10592 , 2023.
[154] H. Liu, C. Li, Q. Wu, and Y . J. Lee, “Visual instruction tuning,” arXiv
preprint arXiv:2304.08485, 2023.
[155] S. J. Rennie, E. Marcheret, Y . Mroueh, J. Ross, and V . Goel, “Self-
critical sequence training for image captioning,” in Proceedings of the
IEEE conference on computer vision and pattern recognition , 2017,
pp. 7008–7024.
[156] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo, “Image captioning
with semantic attention,” in Proceedings of the IEEE conference on
computer vision and pattern recognition , 2016, pp. 4651–4659.
[157] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell:
Metadata: {}
--------------------------------------------------

Chunk 155:
Page Content: [157] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell:
Lessons learned from the 2015 mscoco image captioning challenge,”
IEEE transactions on pattern analysis and machine intelligence ,
vol. 39, no. 4, pp. 652–663, 2016.
[158] M. Z. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga, “A compre-
hensive survey of deep learning for image captioning,”ACM Computing
Surveys (CsUR), vol. 51, no. 6, pp. 1–36, 2019.
[159] P. Wang, Q. Wu, C. Shen, A. Dick, and A. Van Den Hengel, “Fvqa:
Fact-based visual question answering,” IEEE transactions on pattern
analysis and machine intelligence , vol. 40, no. 10, pp. 2413–2427,
2017.
[160] Q. Wu, D. Teney, P. Wang, C. Shen, A. Dick, and A. Van Den Hengel,
“Visual question answering: A survey of methods and datasets,”
Computer Vision and Image Understanding, vol. 163, pp. 21–40, 2017.
[161] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh, “Vqa: Visual question answering,” in Proceedings of the
Metadata: {}
--------------------------------------------------

Chunk 156:
Page Content: D. Parikh, “Vqa: Visual question answering,” in Proceedings of the
IEEE international conference on computer vision , 2015, pp. 2425–
2433.
[162] Y .-L. Sung, J. Cho, and M. Bansal, “Vl-adapter: Parameter-efficient
transfer learning for vision-and-language tasks,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2022, pp. 5227–5237.
[163] Z.-Y . Hu, Y . Li, M. R. Lyu, and L. Wang, “Vl-pet: Vision-and-language
parameter-efficient tuning via granularity control,” in Proceedings of
the IEEE/CVF International Conference on Computer Vision , 2023,
pp. 3010–3020.
[164] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and
Y . Qiao, “Llama-adapter: Efficient fine-tuning of language models with
zero-init attention,” arXiv preprint arXiv:2303.16199 , 2023.
[165] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu,
C. He, X. Yue et al. , “Llama-adapter v2: Parameter-efficient visual
Metadata: {}
--------------------------------------------------

Chunk 157:
Page Content: C. He, X. Yue et al. , “Llama-adapter v2: Parameter-efficient visual
instruction model,” arXiv preprint arXiv:2304.15010 , 2023.
[166] B. Zhao, H. Tu, C. Wei, J. Mei, and C. Xie, “Tuning layernorm in
attention: Towards efficient multi-modal llm finetuning,” arXiv preprint
arXiv:2312.11420, 2023.
[167] S. Lee, “Toward continual learning for conversational agents,” arXiv
preprint arXiv:1712.09943, 2017.
[168] C.-H. Chang, M. Kayed, M. R. Girgis, and K. F. Shaalan, “A survey of
web information extraction systems,” IEEE transactions on knowledge
and data engineering , vol. 18, no. 10, pp. 1411–1428, 2006.
[169] W. Yang, Y . Xie, A. Lin, X. Li, L. Tan, K. Xiong, M. Li, and J. Lin,
“End-to-end open-domain question answering with bertserini,” arXiv
preprint arXiv:1902.01718, 2019.
[170] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins,
A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska
et al. , “Overcoming catastrophic forgetting in neural networks,” Pro-
Metadata: {}
--------------------------------------------------

Chunk 158:
Page Content: et al. , “Overcoming catastrophic forgetting in neural networks,” Pro-
ceedings of the national academy of sciences , vol. 114, no. 13, pp.
3521–3526, 2017.
[171] A. Madotto, Z. Lin, Z. Zhou, S. Moon, P. Crook, B. Liu, Z. Yu, E. Cho,
and Z. Wang, “Continual learning in task-oriented dialogue systems,”
arXiv preprint arXiv:2012.15504 , 2020.
[172] Q. Zhu, B. Li, F. Mi, X. Zhu, and M. Huang, “Continual prompt tuning
for dialog state tracking,” arXiv preprint arXiv:2203.06654 , 2022.
[173] Y . Dai, H. Lang, Y . Zheng, F. Huang, L. Si, and Y . Li, “Lifelong
learning for question answering with hierarchical prompts,” arXiv
preprint arXiv:2208.14602, 2022.
[174] Z. Liang, F. Wei, Y . Jie, Y . Qian, Z. Hao, and B. Han, “Prompts can
play lottery tickets well: Achieving lifelong information extraction via
lottery prompt tuning,” in Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long
Papers), 2023, pp. 277–292.
Metadata: {}
--------------------------------------------------

Chunk 159:
Page Content: of the Association for Computational Linguistics (Volume 1: Long
Papers), 2023, pp. 277–292.
[175] X. Wang, T. Chen, Q. Ge, H. Xia, R. Bao, R. Zheng, Q. Zhang,
T. Gui, and X. Huang, “Orthogonal subspace learning for language
model continual learning,” arXiv preprint arXiv:2310.14152 , 2023.
[176] S. Chen, S. Wong, L. Chen, and Y . Tian, “Extending context window
of large language models via positional interpolation,” arXiv preprint
arXiv:2306.15595, 2023.
[177] Y . Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, and J. Jia, “Longlora:
Efficient fine-tuning of long-context large language models,” arXiv
preprint arXiv:2309.12307, 2023.
[178] J. Yang, “Longqlora: Efficient and effective method to extend context
length of large language models,” arXiv preprint arXiv:2311.04879 ,
2023.
[179] S. Tan, X. Li, S. Patil, Z. Wu, T. Zhang, K. Keutzer, J. E. Gonzalez,
and R. A. Popa, “Lloco: Learning long contexts offline,” arXiv preprint
arXiv:2404.07979, 2024.
Metadata: {}
--------------------------------------------------

Chunk 160:
Page Content: and R. A. Popa, “Lloco: Learning long contexts offline,” arXiv preprint
arXiv:2404.07979, 2024.
[180] Z. Zhang, Y . Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song,
Y . Tian, C. R´e, C. Barrett et al., “H2o: Heavy-hitter oracle for efficient
generative inference of large language models,” Advances in Neural
Information Processing Systems , vol. 36, 2024.
[181] Y . Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang,
C. R ´e, I. Stoica, and C. Zhang, “Flexgen: High-throughput generative
inference of large language models with a single gpu,” in International
Conference on Machine Learning . PMLR, 2023, pp. 31 094–31 116.
[182] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer, “Gpt3. int8
(): 8-bit matrix multiplication for transformers at scale,” Advances in
Neural Information Processing Systems , vol. 35, pp. 30 318–30 332,
2022.
[183] H. Kang, Q. Zhang, S. Kundu, G. Jeong, Z. Liu, T. Krishna, and
T. Zhao, “Gear: An efficient kv cache compression recipefor near-
Metadata: {}
--------------------------------------------------

Chunk 161:
Page Content: T. Zhao, “Gear: An efficient kv cache compression recipefor near-
lossless generative inference of llm,” arXiv preprint arXiv:2403.05527,
2024.
[184] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,
“An image is worth 16x16 words: Transformers for image recognition
at scale. arxiv 2020,” arXiv preprint arXiv:2010.11929 , 2010.
24
[185] A. Steiner, A. Kolesnikov, X. Zhai, R. Wightman, J. Uszkoreit, and
L. Beyer, “How to train your vit? data, augmentation, and regularization
in vision transformers,” arXiv preprint arXiv:2106.10270 , 2021.
[186] X. Chen, S. Xie, and K. He, “An empirical study of training self-
supervised vision transformers,” in Proceedings of the IEEE/CVF
international conference on computer vision , 2021, pp. 9640–9649.
[187] K. He, X. Chen, S. Xie, Y . Li, P. Doll ´ar, and R. Girshick, “Masked
autoencoders are scalable vision learners,” in Proceedings of the
Metadata: {}
--------------------------------------------------

Chunk 162:
Page Content: autoencoders are scalable vision learners,” in Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition ,
2022, pp. 16 000–16 009.
[188] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer,
A. P. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin et al., “Scaling
vision transformers to 22 billion parameters,” in International Confer-
ence on Machine Learning . PMLR, 2023, pp. 7480–7512.
[189] Z. Chen, Y . Duan, W. Wang, J. He, T. Lu, J. Dai, and Y . Qiao,
“Vision transformer adapter for dense predictions,” arXiv preprint
arXiv:2205.08534, 2022.
[190] Z. Wang, Z. Zhang, C.-Y . Lee, H. Zhang, R. Sun, X. Ren, G. Su,
V . Perot, J. Dy, and T. Pfister, “Learning to prompt for continual
learning,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2022, pp. 139–149.
[191] Q. Gao, C. Zhao, Y . Sun, T. Xi, G. Zhang, B. Ghanem, and J. Zhang, “A
unified continual learning framework with general parameter-efficient
Metadata: {}
--------------------------------------------------

Chunk 163:
Page Content: unified continual learning framework with general parameter-efficient
tuning,” arXiv preprint arXiv:2303.10070 , 2023.
[192] L. Ren, C. Chen, L. Wang, and K. Hua, “Learning semantic proxies
from visual prompts for parameter-efficient fine-tuning in deep metric
learning,” arXiv preprint arXiv:2402.02340 , 2024.
[193] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan,
and S.-N. Lim, “Visual prompt tuning,” in European Conference on
Computer Vision. Springer, 2022, pp. 709–727.
[194] S. Chen, C. Ge, Z. Tong, J. Wang, Y . Song, J. Wang, and P. Luo,
“Adaptformer: Adapting vision transformers for scalable visual recog-
nition,” Advances in Neural Information Processing Systems , vol. 35,
pp. 16 664–16 678, 2022.
[195] S. Jie and Z.-H. Deng, “Convolutional bypasses are better vision
transformer adapters,” arXiv preprint arXiv:2207.07039 , 2022.
[196] S. Yoo, E. Kim, D. Jung, J. Lee, and S. Yoon, “Improving visual
Metadata: {}
--------------------------------------------------

Chunk 164:
Page Content: [196] S. Yoo, E. Kim, D. Jung, J. Lee, and S. Yoon, “Improving visual
prompt tuning for self-supervised vision transformers,” arXiv preprint
arXiv:2306.05067, 2023.
[197] J. Pan, Z. Lin, X. Zhu, J. Shao, and H. Li, “St-adapter: Parameter-
efficient image-to-video transfer learning,” Advances in Neural Infor-
mation Processing Systems , vol. 35, pp. 26 462–26 477, 2022.
[198] T. Yang, Y . Zhu, Y . Xie, A. Zhang, C. Chen, and M. Li, “Aim: Adapting
image models for efficient video action recognition,” arXiv preprint
arXiv:2302.03024, 2023.
[199] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable
visual models from natural language supervision,” in International
conference on machine learning . PMLR, 2021, pp. 8748–8763.
[200] C. Jia, Y . Yang, Y . Xia, Y .-T. Chen, Z. Parekh, H. Pham, Q. Le, Y .-H.
Sung, Z. Li, and T. Duerig, “Scaling up visual and vision-language
Metadata: {}
--------------------------------------------------

Chunk 165:
Page Content: Sung, Z. Li, and T. Duerig, “Scaling up visual and vision-language
representation learning with noisy text supervision,” in International
conference on machine learning . PMLR, 2021, pp. 4904–4916.
[201] Y . Li, F. Liang, L. Zhao, Y . Cui, W. Ouyang, J. Shao, F. Yu, and J. Yan,
“Supervision exists everywhere: A data efficient contrastive language-
image pre-training paradigm,” arXiv preprint arXiv:2110.05208, 2021.
[202] A. Singh, R. Hu, V . Goswami, G. Couairon, W. Galuba, M. Rohrbach,
and D. Kiela, “Flava: A foundational language and vision alignment
model,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2022, pp. 15 638–15 650.
[203] M. Xu, Z. Zhang, F. Wei, H. Hu, and X. Bai, “Side adapter network
for open-vocabulary semantic segmentation,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2023, pp. 2945–2954.
[204] Q. Yu, J. He, X. Deng, X. Shen, and L.-C. Chen, “Convolutions die
Metadata: {}
--------------------------------------------------

Chunk 166:
Page Content: 2023, pp. 2945–2954.
[204] Q. Yu, J. He, X. Deng, X. Shen, and L.-C. Chen, “Convolutions die
hard: Open-vocabulary segmentation with single frozen convolutional
clip,” arXiv preprint arXiv:2308.02487 , 2023.
[205] Z. Xu, Z. Chen, Y . Zhang, Y . Song, X. Wan, and G. Li, “Bridging
vision and language encoders: Parameter-efficient tuning for referring
image segmentation,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision , 2023, pp. 17 503–17 512.
[206] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y . Qiao,
P. Gao, and H. Li, “Pointclip: Point cloud understanding by clip,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2022, pp. 8552–8562.
[207] X. Zhu, R. Zhang, B. He, Z. Guo, Z. Zeng, Z. Qin, S. Zhang,
and P. Gao, “Pointclip v2: Prompting clip and gpt for powerful 3d
open-world learning,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision , 2023, pp. 2639–2650.
Metadata: {}
--------------------------------------------------

Chunk 167:
Page Content: Conference on Computer Vision , 2023, pp. 2639–2650.
[208] Z. Wang, X. Yu, Y . Rao, J. Zhou, and J. Lu, “P2p: Tuning pre-trained
image models for point cloud analysis with point-to-pixel prompt-
ing,” Advances in neural information processing systems , vol. 35, pp.
14 388–14 402, 2022.
[209] T. Huang, B. Dong, Y . Yang, X. Huang, R. W. Lau, W. Ouyang, and
W. Zuo, “Clip2point: Transfer clip to point cloud classification with
image-depth pre-training,” in Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , 2023, pp. 22 157–22 167.
[210] C. Ju, T. Han, K. Zheng, Y . Zhang, and W. Xie, “Prompting visual-
language models for efficient video understanding,” in European Con-
ference on Computer Vision . Springer, 2022, pp. 105–124.
[211] B. Ni, H. Peng, M. Chen, S. Zhang, G. Meng, J. Fu, S. Xiang, and
H. Ling, “Expanding language-image pretrained models for general
video recognition,” in European Conference on Computer Vision .
Springer, 2022, pp. 1–18.
Metadata: {}
--------------------------------------------------

Chunk 168:
Page Content: video recognition,” in European Conference on Computer Vision .
Springer, 2022, pp. 1–18.
[212] Z. Lin, S. Geng, R. Zhang, P. Gao, G. de Melo, X. Wang, J. Dai,
Y . Qiao, and H. Li, “Frozen clip models are efficient video learners,”
in European Conference on Computer Vision . Springer, 2022, pp.
388–404.
[213] Z. Han, F. Zhu, Q. Lao, and H. Jiang, “Zero-shot referring expression
comprehension via structural similarity between images and captions,”
arXiv preprint arXiv:2311.17048 , 2023.
[214] S. Doveh, A. Arbelle, S. Harary, E. Schwartz, R. Herzig, R. Giryes,
R. Feris, R. Panda, S. Ullman, and L. Karlinsky, “Teaching structured
vision & language concepts to vision & language models,” in Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2023, pp. 2657–2668.
[215] S. Nag, X. Zhu, Y .-Z. Song, and T. Xiang, “Zero-shot temporal action
detection via vision-language prompting,” in European Conference on
Computer Vision. Springer, 2022, pp. 681–697.
Metadata: {}
--------------------------------------------------

Chunk 169:
Page Content: Computer Vision. Springer, 2022, pp. 681–697.
[216] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Learning to prompt for
vision-language models,” International Journal of Computer Vision ,
vol. 130, no. 9, pp. 2337–2348, 2022.
[217] ——, “Conditional prompt learning for vision-language models,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2022, pp. 16 816–16 825.
[218] B. Zhu, Y . Niu, Y . Han, Y . Wu, and H. Zhang, “Prompt-aligned gradient
for prompt tuning,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision , 2023, pp. 15 659–15 669.
[219] M. U. Khattak, H. Rasheed, M. Maaz, S. Khan, and F. S. Khan, “Maple:
Multi-modal prompt learning,” in Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, 2023, pp. 19 113–
19 122.
[220] M. Shu, W. Nie, D.-A. Huang, Z. Yu, T. Goldstein, A. Anandkumar,
and C. Xiao, “Test-time prompt tuning for zero-shot generalization in
Metadata: {}
--------------------------------------------------

Chunk 170:
Page Content: and C. Xiao, “Test-time prompt tuning for zero-shot generalization in
vision-language models,” Advances in Neural Information Processing
Systems, vol. 35, pp. 14 274–14 289, 2022.
[221] C.-M. Feng, K. Yu, Y . Liu, S. Khan, and W. Zuo, “Diverse data
augmentation with diffusions for effective test-time prompt tuning,” in
Proceedings of the IEEE/CVF International Conference on Computer
Vision, 2023, pp. 2704–2714.
[222] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y . Zhang, H. Li, and
Y . Qiao, “Clip-adapter: Better vision-language models with feature
adapters,” International Journal of Computer Vision , pp. 1–15, 2023.
[223] R. Zhang, R. Fang, W. Zhang, P. Gao, K. Li, J. Dai, Y . Qiao,
and H. Li, “Tip-adapter: Training-free clip-adapter for better vision-
language modeling,” arXiv preprint arXiv:2111.03930 , 2021.
[224] E. Orhan, “A simple cache model for image recognition,” Advances in
Neural Information Processing Systems , vol. 31, 2018.
Metadata: {}
--------------------------------------------------

Chunk 171:
Page Content: Neural Information Processing Systems , vol. 31, 2018.
[225] E. Grave, M. M. Cisse, and A. Joulin, “Unbounded cache model for
online language modeling with open vocabulary,” Advances in neural
information processing systems , vol. 30, 2017.
[226] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic
models,” Advances in neural information processing systems , vol. 33,
pp. 6840–6851, 2020.
[227] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli,
“Deep unsupervised learning using nonequilibrium thermodynamics,”
in International conference on machine learning . PMLR, 2015, pp.
2256–2265.
[228] Z. Han, Y . Wang, L. Zhou, P. Wang, B. Yan, J. Zhou, Y . Wang,
and D. Shen, “Contrastive diffusion model with auxiliary guidance for
coarse-to-fine pet reconstruction,” in International Conference on Med-
ical Image Computing and Computer-Assisted Intervention . Springer,
2023, pp. 239–249.
25
[229] L. Yang, Z. Zhang, Y . Song, S. Hong, R. Xu, Y . Zhao, W. Zhang,
Metadata: {}
--------------------------------------------------

Chunk 172:
Page Content: 2023, pp. 239–249.
25
[229] L. Yang, Z. Zhang, Y . Song, S. Hong, R. Xu, Y . Zhao, W. Zhang,
B. Cui, and M.-H. Yang, “Diffusion models: A comprehensive survey
of methods and applications,” ACM Computing Surveys, vol. 56, no. 4,
pp. 1–39, 2023.
[230] F.-A. Croitoru, V . Hondru, R. T. Ionescu, and M. Shah, “Diffusion
models in vision: A survey,” IEEE Transactions on Pattern Analysis
and Machine Intelligence , 2023.
[231] P. Dhariwal and A. Nichol, “Diffusion models beat gans on image
synthesis,” Advances in neural information processing systems, vol. 34,
pp. 8780–8794, 2021.
[232] N. Ruiz, Y . Li, V . Jampani, Y . Pritch, M. Rubinstein, and K. Aberman,
“Dreambooth: Fine tuning text-to-image diffusion models for subject-
driven generation,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2023, pp. 22 500–22 510.
[233] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,
“High-resolution image synthesis with latent diffusion models,” in
Metadata: {}
--------------------------------------------------

Chunk 173:
Page Content: “High-resolution image synthesis with latent diffusion models,” in
Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, 2022, pp. 10 684–10 695.
[234] S. Luo, Y . Tan, S. Patil, D. Gu, P. von Platen, A. Passos, L. Huang,
J. Li, and H. Zhao, “Lcm-lora: A universal stable-diffusion acceleration
module,” arXiv preprint arXiv:2311.05556 , 2023.
[235] W. Chai, D. Zheng, J. Cao, Z. Chen, C. Wang, and C. Ma, “Speedupnet:
A plug-and-play hyper-network for accelerating text-to-image diffusion
models,” arXiv preprint arXiv:2312.08887 , 2023.
[236] J. Z. Wu, Y . Ge, X. Wang, S. W. Lei, Y . Gu, Y . Shi, W. Hsu, Y . Shan,
X. Qie, and M. Z. Shou, “Tune-a-video: One-shot tuning of image
diffusion models for text-to-video generation,” in Proceedings of the
IEEE/CVF International Conference on Computer Vision , 2023, pp.
7623–7633.
[237] Z. Xing, Q. Dai, H. Hu, Z. Wu, and Y .-G. Jiang, “Simda: Sim-
ple diffusion adapter for efficient video generation,” arXiv preprint
Metadata: {}
--------------------------------------------------

Chunk 174:
Page Content: ple diffusion adapter for efficient video generation,” arXiv preprint
arXiv:2308.09710, 2023.
[238] B. Zeng, S. Li, Y . Feng, H. Li, S. Gao, J. Liu, H. Li, X. Tang, J. Liu, and
B. Zhang, “Ipdreamer: Appearance-controllable 3d object generation
with image prompts,” arXiv preprint arXiv:2310.05375 , 2023.
[239] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y . Hasson,
K. Lenc, A. Mensch, K. Millican, M. Reynolds et al. , “Flamingo:
a visual language model for few-shot learning,” Advances in Neural
Information Processing Systems , vol. 35, pp. 23 716–23 736, 2022.
[240] L. Zhang, A. Rao, and M. Agrawala, “Adding conditional control
to text-to-image diffusion models,” in Proceedings of the IEEE/CVF
International Conference on Computer Vision , 2023, pp. 3836–3847.
[241] R. Gandikota, J. Materzynska, T. Zhou, A. Torralba, and D. Bau, “Con-
cept sliders: Lora adaptors for precise control in diffusion models,”
arXiv preprint arXiv:2311.12092 , 2023.
Metadata: {}
--------------------------------------------------

Chunk 175:
Page Content: arXiv preprint arXiv:2311.12092 , 2023.
[242] C. Mou, X. Wang, L. Xie, Y . Wu, J. Zhang, Z. Qi, Y . Shan, and X. Qie,
“T2i-adapter: Learning adapters to dig out more controllable ability
for text-to-image diffusion models,” arXiv preprint arXiv:2302.08453 ,
2023.
[243] R. Gal, Y . Alaluf, Y . Atzmon, O. Patashnik, A. H. Bermano,
G. Chechik, and D. Cohen-Or, “An image is worth one word: Personal-
izing text-to-image generation using textual inversion,” arXiv preprint
arXiv:2208.01618, 2022.
[244] N. Kumari, B. Zhang, R. Zhang, E. Shechtman, and J.-Y . Zhu, “Multi-
concept customization of text-to-image diffusion,” inProceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2023, pp. 1931–1941.
[245] H. Ye, J. Zhang, S. Liu, X. Han, and W. Yang, “Ip-adapter: Text
compatible image prompt adapter for text-to-image diffusion models,”
arXiv preprint arXiv:2308.06721 , 2023.
[246] OpenAI, “Gpt-4,” in https://openai.com/gpt-4, 2023.
Metadata: {}
--------------------------------------------------

Chunk 176:
Page Content: arXiv preprint arXiv:2308.06721 , 2023.
[246] OpenAI, “Gpt-4,” in https://openai.com/gpt-4, 2023.
[247] G. Team, R. Anil, S. Borgeaud, Y . Wu, J.-B. Alayrac, J. Yu, R. Soricut,
J. Schalkwyk, A. M. Dai, A. Hauth et al., “Gemini: a family of highly
capable multimodal models,” arXiv preprint arXiv:2312.11805 , 2023.
[248] Z. Zhou, X. Wei, J. Zhang, and G. Sun, “ tPetSu: A unified framework
for tParameter-Efficientutransformers serving,” in 2022 USENIX An-
nual Technical Conference (USENIX ATC 22) , 2022, pp. 489–504.
[249] B. Wu, R. Zhu, Z. Zhang, P. Sun, X. Liu, and X. Jin, “ tdLoRAu:
Dynamically orchestrating requests and adapters for tLoRAutLLMu
serving,” in 18th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 24) , 2024, pp. 911–927.
[250] C. Gao and S. Q. Zhang, “Dlora: Distributed parameter-efficient
fine-tuning solution for large language model,” arXiv preprint
arXiv:2404.05182, 2024.
Metadata: {}
--------------------------------------------------

Chunk 177:
Page Content: fine-tuning solution for large language model,” arXiv preprint
arXiv:2404.05182, 2024.
[251] G. Xiao, J. Lin, and S. Han, “Offsite-tuning: Transfer learning without
full model,” arXiv preprint arXiv:2302.04870 , 2023.
[252] L. Chen, Z. Ye, Y . Wu, D. Zhuo, L. Ceze, and A. Krishnamurthy,
“Punica: Multi-tenant lora serving,” arXiv preprint arXiv:2310.18547 ,
2023.
[253] S. Mangrulkar, S. Gugger, L. Debut, Y . Belkada, S. Paul, and
B. Bossan, “Peft: State-of-the-art parameter-efficient fine-tuning meth-
ods,” https://github.com/huggingface/peft, 2022.
[254] C. Poth, H. Sterz, I. Paul, S. Purkayastha, L. Engl ¨ander, T. Imhof,
I. Vuli ´c, S. Ruder, I. Gurevych, and J. Pfeiffer, “Adapters: A unified
library for parameter-efficient and modular transfer learning,” 2023.
[255] K. Chen, J. Wang, J. Pang, Y . Cao, Y . Xiong, X. Li, S. Sun, W. Feng,
Z. Liu, J. Xu, Z. Zhang, D. Cheng, C. Zhu, T. Cheng, Q. Zhao, B. Li,
X. Lu, R. Zhu, Y . Wu, J. Dai, J. Wang, J. Shi, W. Ouyang, C. C.
Metadata: {}
--------------------------------------------------

Chunk 178:
Page Content: X. Lu, R. Zhu, Y . Wu, J. Dai, J. Wang, J. Shi, W. Ouyang, C. C.
Loy, and D. Lin, “MMDetection: Open mmlab detection toolbox and
benchmark,” arXiv preprint arXiv:1906.07155 , 2019.
[256] S. Q. Zhang, T. Tambe, N. Cuevas, G.-Y . Wei, and D. Brooks, “Camel:
Co-designing ai models and embedded drams for efficient on-device
learning,” arXiv preprint arXiv:2305.03148 , 2023.
[257] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y . Guo,
L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman,
C. Ng, R. Wang, and A. Ramesh, “Video generation
models as world simulators,” 2024. [Online]. Available: https:
//openai.com/research/video-generation-models-as-world-simulators
[258] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with
selective state spaces,” arXiv preprint arXiv:2312.00752 , 2023.
[259] Y . Bai, X. Geng, K. Mangalam, A. Bar, A. Yuille, T. Darrell, J. Malik,
and A. A. Efros, “Sequential modeling enables scalable learning for
Metadata: {}
--------------------------------------------------

Chunk 179:
Page Content: and A. A. Efros, “Sequential modeling enables scalable learning for
large vision models,” arXiv preprint arXiv:2312.00785 , 2023.
[260] A. Dosovitskiy and T. Brox, “Inverting visual representations with
convolutional networks,” in Proceedings of the IEEE conference on
computer vision and pattern recognition , 2016, pp. 4829–4837.
[261] Z. He, T. Zhang, and R. B. Lee, “Model inversion attacks against
collaborative inference,” in Proceedings of the 35th Annual Computer
Security Applications Conference , 2019, pp. 148–162.
Metadata: {}
--------------------------------------------------

Chunk 180:
Page Content: # Fine-Tuning using LoRA and QLoRA

Fine tuning updates all the parameters of a pre-trained model to adapt it for a specific task, offering high accuracy but requiring significant computational resources and memory. In contrast, LoRA (Low-Rank Adaptation) is a parameter-efficient technique that introduces small trainable matrices into certain layers, allowing most of the original model parameters to remain unchanged. This approach drastically reduces memory and compute requirements, making LoRA much faster and more efficient than full finetuning, especially for large language models, while still achieving comparable performance on many tasks.

## What is Fine-Tuning?
Metadata: {}
--------------------------------------------------

Chunk 181:
Page Content: ## What is Fine-Tuning?

Fine-tuning involves taking a pre-trained language model (which has already learned a vast amount of general knowledge and language patterns from a diverse dataset) and further training it on a smaller, task-specific, or domain-specific dataset. This process “adjusts” the model’s internal parameters (weights) to better understand and generate text aligned with the nuances, jargon, style, or specific tasks of the new data.

### Traditional Fine-Tuning

In its original form, fine-tuning involves updating all, or a significant portion, of a pre-trained model’s parameters. For models with hundreds of millions or even billions of parameters, this is a computationally intensive and resource-demanding process. It requires substantial GPU power, memory, and time, making it less practical for frequent updates or for users with limited hardware. The output is a new, specialized version of the entire model.

## LoRA (Low-Rank Adaptation)

### How LoRA Works
Metadata: {}
--------------------------------------------------

Chunk 182:
Page Content: ## LoRA (Low-Rank Adaptation)

### How LoRA Works

The below image shows a transformer block architecture where adapters are inserted after the feed-forward network and before layer normalization. In LoRA, these adapters are implemented as low-rank matrices. During fine-tuning, only these adapter parameters are updated, while the core model weights (multi-head attention, feed-forward network, etc.) stay fixed.

- **Adapters in the Stack:** Each transformer block contains a small adapter module.
- **Parameter Update:** Only the adapters (the low-rank matrices) are updated during training, greatly reducing the number of trainable parameters.

## Key Features of LoRA (Low-Rank Adaptation) for Fine-Tuning Large Models
Metadata: {}
--------------------------------------------------

Chunk 183:
Page Content: - **Parameter-Efficient Fine-Tuning**: LoRA introduces small, trainable low-rank matrices into specific layers of a pre-trained model, allowing only these adapters to be updated during fine-tuning, while the vast majority of the model's parameters remain frozen.
- **Reduced Trainable Parameters**: Typically, only 0.5–5% of the model’s parameters are updated, as opposed to 100% in full finetuning. This makes LoRA much faster and less resource intensive.
- **Memory Efficiency**: LoRA significantly reduces memory and hardware requirements. For example, a 1GB model may need just 2GB of VRAM for LoRA finetuning, compared to 16GB+ for full finetuning.
- **Implementation Simplicity**: LoRA is widely supported in libraries like HuggingFace PEFT, making it easy to integrate into existing workflows.
- **Low Overfitting Risk**: By training fewer parameters, LoRA helps avoid overfitting, especially with smaller datasets.
Metadata: {}
--------------------------------------------------

Chunk 184:
Page Content: - **Modularity**: LoRA adapters can be swapped in and out for different tasks, enabling flexible multi-task deployment without retraining the entire model.
- **No Inference Latency**: Once fine-tuned, LoRA adapters can be merged into the main model weights, so there is no additional inference cost.
Metadata: {}
--------------------------------------------------

Chunk 185:
Page Content: ## QLoRA (Quantized LoRA)

QLoRA (Quantized LoRA) works by loading the base language model in a highly compressed 4-bit quantized format, drastically reducing memory usage, while training small LoRA adapters in higher precision. During fine-tuning, only these adapters are updated, compensating for any quantization errors and preserving model performance. This approach allows you to efficiently fine-tune massive models on standard GPUs, combining aggressive memory savings with the parameter efficiency of LoRA and maintaining competitive results.

**Training Process**:

- The pretrained model is loaded with quantized 4-bit weights.
- Only the LoRA adapters are updated during training.
- Libraries like BitsAndBytes (for quantization) and PEFT (for LoRA) are used together for implementation.

## Key Features of QLoRA
Metadata: {}
--------------------------------------------------

Chunk 186:
Page Content: - **Further Memory Reduction**: QLoRA extends LoRA by quantizing the main model weights to 4 bits (using methods like NF4), while keeping the LoRA adapters in higher precision (e.g., 16-bit).
- **Ultra-Low Resource Requirements**: QLoRA can fine-tune very large models (billions of parameters) on consumer-grade GPUs or even CPUs by reducing VRAM needs to as little as 0.5GB per 1GB model.
- **Performance**: QLoRA has been shown to maintain comparable accuracy to standard LoRA and full fine-tuning, even on very large models. In many cases, performance loss is negligible or non-existent.
- **Adapter Placement**: For QLoRA, it is often recommended to apply LoRA adapters to all linear transformer blocks, not just the query/key/value layers, especially for larger models.
- **Double Quantization**: QLoRA may use double quantization techniques to further compress storage, especially for scale/offset constants.
Metadata: {}
--------------------------------------------------

Chunk 187:
Page Content: - **Mitigating Quantization Loss**: QLoRA uses LoRA as an accessory to correct for any errors introduced by quantization, ensuring high accuracy is maintained.
- **Trade-offs**: While QLoRA is slightly slower than LoRA due to quantization/dequantization steps, the memory savings are substantial, and the method is highly scalable.
Metadata: {}
--------------------------------------------------

Chunk 188:
Page Content: ## Benchmarks and Findings

### LoRA vs. Full Fine-Tuning

- LoRA achieves competitive performance relative to full fine-tuning in tasks like text classification, summarization, and question answering, often matching accuracy while training 0.2–0.3% of total parameters. For example, adapter-based LoRA models achieve GLUE scores within 1% of fully fine-tuned models.
- Full fine-tuning retains an advantage in complex domains (e.g., mathematics, programming) where precise parameter adjustments are critical, though this gap narrows with proper hyperparameter tuning.
- LoRA reduces memory usage by 70% compared to full fine-tuning, enabling cost-effective deployment on consumer-grade GPUs.

### Adapters and Efficiency Trade-offs
Metadata: {}
--------------------------------------------------

Chunk 189:
Page Content: ### Adapters and Efficiency Trade-offs

- Adapters deliver robust performance with substantially lower computational requirements, achieving accuracy comparable to full fine-tuning in tasks like sentiment analysis and legal document processing.
- The primary trade-off for adapters is a marginal increase in inference latency (10–20%) due to additional layers processed during prediction.
Metadata: {}
--------------------------------------------------

